# nano-vLLM vs mini-sglang æ·±åº¦å¯¹æ¯”

> ä¸¤ä¸ªè½»é‡çº§ LLM æ¨ç†æ¡†æ¶çš„å…¨æ–¹ä½æ¯”è¾ƒ

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è§ˆå¯¹æ¯”](#1-é¡¹ç›®æ¦‚è§ˆå¯¹æ¯”)
2. [æ¶æ„è®¾è®¡å¯¹æ¯”](#2-æ¶æ„è®¾è®¡å¯¹æ¯”)
3. [æ ¸å¿ƒç»„ä»¶å¯¹æ¯”](#3-æ ¸å¿ƒç»„ä»¶å¯¹æ¯”)
4. [ç¼“å­˜æœºåˆ¶å¯¹æ¯”](#4-ç¼“å­˜æœºåˆ¶å¯¹æ¯”)
5. [è°ƒåº¦ç­–ç•¥å¯¹æ¯”](#5-è°ƒåº¦ç­–ç•¥å¯¹æ¯”)
6. [æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”](#6-æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”)
7. [ä»£ç å¤æ‚åº¦å¯¹æ¯”](#7-ä»£ç å¤æ‚åº¦å¯¹æ¯”)
8. [é€‚ç”¨åœºæ™¯å¯¹æ¯”](#8-é€‚ç”¨åœºæ™¯å¯¹æ¯”)
9. [å­¦ä¹ è·¯å¾„å»ºè®®](#9-å­¦ä¹ è·¯å¾„å»ºè®®)

---

## 1. é¡¹ç›®æ¦‚è§ˆå¯¹æ¯”

### 1.1 åŸºæœ¬ä¿¡æ¯

| ç»´åº¦ | nano-vLLM | mini-sglang |
|------|-----------|-------------|
| **ä»£ç è¡Œæ•°** | ~2,000 è¡Œ | ~5,000 è¡Œ |
| **ä¸»è¦è¯­è¨€** | Python | Python + C++ CUDA |
| **è®¾è®¡ç›®æ ‡** | æ•™å­¦æ€§è´¨ï¼Œç®€åŒ–å®ç° | ç”Ÿäº§å°±ç»ªï¼Œé«˜æ€§èƒ½ |
| **æ”¯æŒæ¨¡å‹** | Qwen2/3 | Llama3, Qwen3 |
| **å¤šGPUæ”¯æŒ** | Tensor Parallelism | Tensor Parallelism |
| **APIæœåŠ¡** | æ—  | OpenAI-compatible API |
| **éƒ¨ç½²æ¨¡å¼** | å•è¿›ç¨‹è„šæœ¬ | å¤šè¿›ç¨‹åˆ†å¸ƒå¼ç³»ç»Ÿ |

### 1.2 é¡¹ç›®å®šä½

**nano-vLLM**ï¼š
```
æ•™å­¦å·¥å…·
â”œâ”€ æ ¸å¿ƒç†è§£ vLLM å·¥ä½œåŸç†
â”œâ”€ ä»£ç ç®€æ´ï¼Œæ˜“äºä¿®æ”¹
â”œâ”€ é€‚åˆå¿«é€ŸåŸå‹å¼€å‘
â””â”€ æœ€ä½³å­¦ä¹ é¡¹ç›®
```

**mini-sglang**ï¼š
```
ç”Ÿäº§ç³»ç»Ÿ
â”œâ”€ å®Œæ•´çš„åœ¨çº¿æœåŠ¡èƒ½åŠ›
â”œâ”€ é«˜æ€§èƒ½ä¼˜åŒ–
â”œâ”€ åˆ†å¸ƒå¼æ¶æ„
â””â”€ å¯ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒ
```

---

## 2. æ¶æ„è®¾è®¡å¯¹æ¯”

### 2.1 è¿›ç¨‹æ¨¡å‹

**nano-vLLMï¼šå•è¿›ç¨‹ + Tensor Parallelism**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLM.generate()              â”‚
â”‚      (ç”¨æˆ·ç›´æ¥è°ƒç”¨ Python API)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLMEngine                   â”‚
â”‚    (ä¸»è¿›ç¨‹ï¼Œç®¡ç†æ‰€æœ‰é€»è¾‘)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Multiprocessingâ”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“          â†“          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Rank 0â”‚  â”‚Rank 1â”‚  â”‚Rank 2â”‚  (Tensor Parallel)
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
```

**mini-sglangï¼šå¤šè¿›ç¨‹åˆ†å¸ƒå¼ç³»ç»Ÿ**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server   â”‚ â† FastAPIï¼ŒOpenAIæ¥å£
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ ZeroMQ
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tokenizer Worker Process    â”‚
â”‚  - æ–‡æœ¬ â†’ Token              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ ZeroMQ
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduler Worker (Rank 0)   â”‚
â”‚  - æ¥æ”¶è¯·æ±‚                   â”‚
â”‚  - è°ƒåº¦å†³ç­–                   â”‚
â”‚  - å¹¿æ’­åˆ°å…¶ä»– Rank            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ NCCL + ZeroMQ
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduler Workers (Rank 1-N)â”‚
â”‚  - æ¯ä¸ª GPU ä¸€ä¸ªè¿›ç¨‹          â”‚
â”‚  - æœ¬åœ° Engine æ‰§è¡Œ           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ ZeroMQ
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Detokenizer Worker Process  â”‚
â”‚  - Token â†’ æ–‡æœ¬               â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server   â”‚ â† æµå¼è¿”å›ç»“æœ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®å·®å¼‚**ï¼š
- **nano-vLLM**ï¼šæ‰€æœ‰ç»„ä»¶åœ¨ä¸€ä¸ªè¿›ç¨‹ï¼Œç®€å•ä½†æ‰©å±•æ€§æœ‰é™
- **mini-sglang**ï¼šè¿›ç¨‹éš”ç¦»ï¼Œç»„ä»¶ç‹¬ç«‹éƒ¨ç½²ï¼Œå¯æ°´å¹³æ‰©å±•

### 2.2 é€šä¿¡æœºåˆ¶

| ç»„ä»¶ | nano-vLLM | mini-sglang |
|------|-----------|-------------|
| **æ§åˆ¶æ¶ˆæ¯** | Python æ–¹æ³•è°ƒç”¨ | ZeroMQ |
| **å¼ é‡æ•°æ®** | Shared Memory | NCCL + ZeroMQ |
| **è¿›ç¨‹é—´é€šä¿¡** | multiprocessing.Queue | ZMQ Socket |
| **æµå¼è¿”å›** | æ—  | æ”¯æŒæµå¼è¾“å‡º |

**ç¤ºä¾‹ï¼šnano-vLLM çš„ç®€å•é€šä¿¡**

```python
# model_runner.py:68-74
def read_shm(self):
    assert self.world_size > 1 and self.rank > 0
    self.event.wait()  # ç®€å•çš„äº‹ä»¶åŒæ­¥
    n = int.from_bytes(self.shm.buf[0:4], "little")
    method_name, *args = pickle.loads(self.shm.buf[4:n+4])
    self.event.clear()
    return method_name, args
```

**ç¤ºä¾‹ï¼šmini-sglang çš„æ¶ˆæ¯ç³»ç»Ÿ**

```python
# message/backend.py
@dataclass
class BatchBackendMsg(BaseBackendMsg):
    """æ‰¹æ¬¡è¯·æ±‚æ¶ˆæ¯ï¼Œæ”¯æŒåºåˆ—åŒ–"""
    uids: List[int]
    req_ids: List[int]
    input_ids: List[int]
    ...
```

---

## 3. æ ¸å¿ƒç»„ä»¶å¯¹æ¯”

### 3.1 ä»£ç ç»“æ„

**nano-vLLMï¼ˆ~2,000 è¡Œï¼‰**ï¼š
```
nanovllm/
â”œâ”€â”€ llm.py                # 100 è¡Œï¼šLLM API
â”œâ”€â”€ config.py             # 50 è¡Œï¼šé…ç½®
â”œâ”€â”€ sampling_params.py    # 30 è¡Œï¼šé‡‡æ ·å‚æ•°
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ llm_engine.py     # 100 è¡Œï¼šä¸»å¼•æ“
â”‚   â”œâ”€â”€ scheduler.py      # 80 è¡Œï¼šè°ƒåº¦å™¨
â”‚   â”œâ”€â”€ model_runner.py   # 250 è¡Œï¼šæ¨¡å‹æ‰§è¡Œ
â”‚   â”œâ”€â”€ block_manager.py  # 120 è¡Œï¼šå—ç®¡ç†
â”‚   â””â”€â”€ sequence.py       # 90 è¡Œï¼šåºåˆ—çŠ¶æ€
â”œâ”€â”€ layers/               # 500 è¡Œï¼šç¥ç»ç½‘ç»œå±‚
â”œâ”€â”€ models/               # 400 è¡Œï¼šæ¨¡å‹å®ç°
â””â”€â”€ utils/                # 200 è¡Œï¼šå·¥å…·å‡½æ•°
```

**mini-sglangï¼ˆ~5,000 è¡Œï¼‰**ï¼š
```
minisgl/
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ api_server.py     # 300 è¡Œï¼šFastAPI æœåŠ¡å™¨
â”‚   â”œâ”€â”€ launch.py         # 200 è¡Œï¼šè¿›ç¨‹å¯åŠ¨
â”‚   â””â”€â”€ args.py           # 150 è¡Œï¼šCLI å‚æ•°
â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ scheduler.py      # 500 è¡Œï¼šè°ƒåº¦å™¨
â”‚   â”œâ”€â”€ prefill.py        # 200 è¡Œï¼šPrefill ç®¡ç†
â”‚   â”œâ”€â”€ decode.py         # 150 è¡Œï¼šDecode ç®¡ç†
â”‚   â”œâ”€â”€ table.py          # 200 è¡Œï¼šé¡µè¡¨ç®¡ç†
â”‚   â”œâ”€â”€ cache.py          # 100 è¡Œï¼šç¼“å­˜æ¥å£
â”‚   â””â”€â”€ utils.py          # 100 è¡Œï¼šå·¥å…·å‡½æ•°
â”œâ”€â”€ kvcache/
â”‚   â”œâ”€â”€ radix_manager.py  # 400 è¡Œï¼šRadix Cache
â”‚   â”œâ”€â”€ naive_manager.py  # 150 è¡Œï¼šæœ´ç´ ç¼“å­˜
â”‚   â”œâ”€â”€ base.py           # 100 è¡Œï¼šç¼“å­˜æ¥å£
â”‚   â””â”€â”€ mha_pool.py       # 200 è¡Œï¼šMHA æ± 
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ engine.py         # 400 è¡Œï¼šæ‰§è¡Œå¼•æ“
â”‚   â”œâ”€â”€ graph.py          # 150 è¡Œï¼šCUDA Graph
â”‚   â””â”€â”€ sample.py         # 100 è¡Œï¼šé‡‡æ ·
â”œâ”€â”€ attention/            # 300 è¡Œï¼šæ³¨æ„åŠ›åç«¯
â”œâ”€â”€ layers/               # 600 è¡Œï¼šç¥ç»ç½‘ç»œå±‚
â”œâ”€â”€ models/               # 500 è¡Œï¼šæ¨¡å‹å®ç°
â”œâ”€â”€ kernel/               # 800 è¡Œï¼šCUDA Kernels
â”œâ”€â”€ message/              # 400 è¡Œï¼šæ¶ˆæ¯å®šä¹‰
â””â”€â”€ tokenizer/            # 300 è¡Œï¼šåˆ†è¯å™¨
```

### 3.2 æ ¸å¿ƒç±»å¯¹æ¯”

#### 3.2.1 åºåˆ—/è¯·æ±‚è¡¨ç¤º

**nano-vLLMï¼šSequence ç±»**

```python
# sequence.py:14-84
class Sequence:
    """ç®€å•çš„åºåˆ—çŠ¶æ€"""
    block_size = 256

    def __init__(self, token_ids: list[int], sampling_params):
        self.token_ids = token_ids          # å®Œæ•´ token åºåˆ—
        self.block_table = []               # å—è¡¨
        self.num_cached_tokens = 0          # ç¼“å­˜å‘½ä¸­æ•°
        self.status = SequenceStatus.WAITING
```

**mini-sglangï¼šReq ç±»**

```python
# core.py
@dataclass
class Req:
    """æ›´ä¸°å¯Œçš„è¯·æ±‚çŠ¶æ€"""
    uid: int                               # å”¯ä¸€ ID
    req_id: int                            # è¯·æ±‚ ID
    input_ids: torch.Tensor                # è¾“å…¥ï¼ˆåœ¨ GPU ä¸Šï¼‰
    parent: Optional[Req] = None           # çˆ¶è¯·æ±‚ï¼ˆç”¨äº Chunked Prefillï¼‰
    filling_status: Literal["prefill", "decode"] = "prefill"

    # é‡‡æ ·å‚æ•°
    sampling_params: SamplingParams = field(default_factory=SamplingParams)

    # çŠ¶æ€ç®¡ç†
    decoded_tokens: List[int] = field(default_factory=list)
    output_ids: torch.Tensor = field(default_factory=lambda: torch.empty(0))
    completion_tokens_wo_eos: int = 0

    # KV Cache å¼•ç”¨
    cache_handle: Optional[BaseCacheHandle] = None
```

**å…³é”®å·®å¼‚**ï¼š
- **nano-vLLM**ï¼šæ•°æ®åœ¨ CPUï¼Œåºåˆ—åŒ–æ—¶ä¼ è¾“
- **mini-sglang**ï¼šæ•°æ®åœ¨ GPUï¼Œé¿å…é¢‘ç¹ä¼ è¾“

#### 3.2.2 ç¼“å­˜ç®¡ç†

**nano-vLLMï¼šBlockManager**

```python
# block_manager.py:26-113
class BlockManager:
    def __init__(self, num_blocks: int, block_size: int):
        self.blocks = [Block(i) for i in range(num_blocks)]
        self.hash_to_block_id = {}          # å“ˆå¸Œ â†’ å—ID
        self.free_block_ids = deque(range(num_blocks))

    def allocate(self, seq: Sequence):
        """åˆ†é…å—ï¼Œæ”¯æŒå‰ç¼€ç¼“å­˜"""
        ...
```

**mini-sglangï¼šRadixCacheManager**

```python
# kvcache/radix_manager.py:87-100
class RadixCacheManager(BaseCacheManager):
    def __init__(self, device: torch.device):
        self.root_node = RadixTreeNode()    # Radix æ ‘æ ¹èŠ‚ç‚¹
        self.evictable_size = 0             # å¯é©±é€å¤§å°
        self.protected_size = 0             # å—ä¿æŠ¤å¤§å°

    def lock_handle(self, handle: BaseCacheHandle, unlock: bool = False):
        """é”å®š/è§£é”ç¼“å­˜å—"""
        ...
```

**å…³é”®å·®å¼‚**ï¼š
- **nano-vLLM**ï¼šå“ˆå¸Œè¡¨æŸ¥æ‰¾ï¼ŒO(1) å¤æ‚åº¦
- **mini-sglang**ï¼šRadix æ ‘ï¼Œæ”¯æŒæœ€é•¿å‰ç¼€åŒ¹é…ï¼Œæ›´çµæ´»

---

## 4. ç¼“å­˜æœºåˆ¶å¯¹æ¯”

### 4.1 nano-vLLMï¼šPagedAttention + å“ˆå¸Œç¼“å­˜

**æ ¸å¿ƒæ€æƒ³**ï¼š
1. å°† KV Cache åˆ†æˆå›ºå®šå¤§å°çš„å—ï¼ˆ256 tokensï¼‰
2. ä½¿ç”¨å“ˆå¸Œè¡¨è®°å½•å·²è®¡ç®—çš„å—
3. æ–°è¯·æ±‚æŸ¥æ‰¾å“ˆå¸Œè¡¨ï¼Œå¤ç”¨ç›¸åŒå—

**ä»£ç ç¤ºä¾‹**ï¼š

```python
# block_manager.py:59-83
def allocate(self, seq: Sequence):
    h = -1
    cache_miss = False

    for i in range(seq.num_blocks):
        token_ids = seq.block(i)

        # è®¡ç®—å“ˆå¸Œï¼ˆæ»šåŠ¨å“ˆå¸Œï¼‰
        if len(token_ids) == self.block_size:
            h = self.compute_hash(token_ids, h)
        else:
            h = -1  # ä¸å®Œæ•´çš„å—ä¸ç¼“å­˜

        # æŸ¥æ‰¾ç¼“å­˜
        block_id = self.hash_to_block_id.get(h, -1)

        # éªŒè¯ï¼ˆé¿å…å“ˆå¸Œå†²çªï¼‰
        if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
            cache_miss = True

        if cache_miss:
            # åˆ†é…æ–°å—
            block_id = self.free_block_ids[0]
            block = self._allocate_block(block_id)
        else:
            # å‘½ä¸­ç¼“å­˜ï¼
            seq.num_cached_tokens += self.block_size
            block = self.blocks[block_id]
            block.ref_count += 1
```

**ç‰¹ç‚¹**ï¼š
- âœ… ç®€å•é«˜æ•ˆï¼ŒO(1) æŸ¥æ‰¾
- âœ… å¼•ç”¨è®¡æ•°æ”¯æŒå…±äº«
- âŒ åªæ”¯æŒå®Œæ•´å—åŒ¹é…
- âŒ å“ˆå¸Œå†²çªéœ€è¦éªŒè¯

### 4.2 mini-sglangï¼šRadix Cache

**æ ¸å¿ƒæ€æƒ³**ï¼š
1. å°†æ‰€æœ‰è¯·æ±‚çš„ KV Cache ç»„ç»‡æˆ Radix æ ‘
2. æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ª token å‰ç¼€
3. æ”¯æŒéƒ¨åˆ†åŒ¹é…å’Œè‡ªåŠ¨åˆ†è£‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
# kvcache/radix_manager.py:13-80
class RadixTreeNode:
    def __init__(self, tic: int | None = None):
        self.children: Dict[int, RadixTreeNode] = {}  # å­èŠ‚ç‚¹
        self._parent: RadixTreeNode | None = None
        self.ref_count: int = 0

        # KV Cache æ•°æ®
        self._key: torch.Tensor      # Key cache
        self._value: torch.Tensor    # Value cache
        self._length: int            # èŠ‚ç‚¹é•¿åº¦

    def get_match_len(self, input_ids: torch.Tensor) -> int:
        """è®¡ç®—ä¸è¾“å…¥çš„åŒ¹é…é•¿åº¦"""
        from minisgl.kernel import fast_compare_key
        return fast_compare_key(self._key, input_ids)

    def _split_at(self, pos: int) -> RadixTreeNode:
        """åœ¨ä½ç½® pos åˆ†è£‚èŠ‚ç‚¹"""
        assert 0 < pos < self.length
        parent = self.parent

        # åˆ›å»ºæ–°èŠ‚ç‚¹ï¼ˆå‰åŠéƒ¨åˆ†ï¼‰
        new_node = RadixTreeNode(self.timestamp)
        new_node.set_key_value(self._key[:pos], self._value[:pos])
        new_node.set_parent(parent)
        new_node.ref_count = self.ref_count

        # å½“å‰èŠ‚ç‚¹ä¿ç•™ååŠéƒ¨åˆ†
        self.set_key_value(self._key[pos:], self._value[pos:])
        self.set_parent(new_node)

        return new_node
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
# åœºæ™¯ï¼šä¸¤ä¸ªè¯·æ±‚å…±äº«å‰ç¼€
req1_tokens = [1, 2, 3, 4, 5, 6]
req2_tokens = [1, 2, 3, 7, 8, 9]

# Radix æ ‘ç»“æ„ï¼š
# Root
#  â””â”€ [1,2,3]           â† å…±äº«èŠ‚ç‚¹ï¼Œref_count=2
#      â”œâ”€ [4,5,6]       â† req1 çš„ç‹¬æœ‰éƒ¨åˆ†
#      â””â”€ [7,8,9]       â† req2 çš„ç‹¬æœ‰éƒ¨åˆ†
```

**ç‰¹ç‚¹**ï¼š
- âœ… æ”¯æŒéƒ¨åˆ†å‰ç¼€åŒ¹é…ï¼ˆæ›´çµæ´»ï¼‰
- âœ… è‡ªåŠ¨åˆ†è£‚å’Œåˆå¹¶
- âœ… å†…å­˜åˆ©ç”¨ç‡æ›´é«˜
- âŒ å®ç°å¤æ‚åº¦é«˜
- âŒ æŸ¥æ‰¾ O(tree_depth) å¤æ‚åº¦

### 4.3 æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | nano-vLLM | mini-sglang |
|------|-----------|-------------|
| **å®Œå…¨åŒ¹é…** | O(1)ï¼Œæå¿« | O(depth)ï¼Œè¾ƒå¿« |
| **éƒ¨åˆ†åŒ¹é…** | ä¸æ”¯æŒ | O(depth)ï¼Œè‡ªåŠ¨å¤„ç† |
| **å†…å­˜å¼€é”€** | å“ˆå¸Œè¡¨ | Radix æ ‘ |
| **ç¼“å­˜å‘½ä¸­ç‡** | ä¸­ç­‰ | é«˜ï¼ˆæ›´ç»†ç²’åº¦ï¼‰ |

**ç¤ºä¾‹ï¼šéƒ¨åˆ†åŒ¹é…**

```python
# nano-vLLMï¼šæ— æ³•åˆ©ç”¨éƒ¨åˆ†åŒ¹é…
seq1 = [1, 2, 3, 4, 5, 6, 7, 8]  # å— 0: [1-8]
seq2 = [1, 2, 3, 9, 10, 11, 12]  # å— 0: [1-7] ä¸åŒ¹é…
# ç»“æœï¼šç¼“å­˜æœªå‘½ä¸­ï¼Œå®Œå…¨é‡æ–°è®¡ç®—

# mini-sglangï¼šè‡ªåŠ¨åŒ¹é…æœ€é•¿å‰ç¼€
req1 = [1, 2, 3, 4, 5, 6, 7, 8]
req2 = [1, 2, 3, 9, 10, 11, 12]
# ç»“æœï¼šè‡ªåŠ¨åŒ¹é… [1,2,3]ï¼Œåªéœ€è®¡ç®— [4,5,6,7,8] å’Œ [9,10,11,12]
```

---

## 5. è°ƒåº¦ç­–ç•¥å¯¹æ¯”

### 5.1 nano-vLLMï¼šç®€å•æŠ¢å è°ƒåº¦

**ä»£ç **ï¼š`scheduler.py:24-58`

```python
def schedule(self) -> tuple[list[Sequence], bool]:
    # é˜¶æ®µ1ï¼šPrefillï¼ˆä¼˜å…ˆå¤„ç†æ–°è¯·æ±‚ï¼‰
    while self.waiting and num_seqs < self.max_num_seqs:
        seq = self.waiting[0]

        # æ£€æŸ¥èµ„æº
        if num_batched_tokens + len(seq) > self.max_num_batched_tokens:
            break
        if not self.block_manager.can_allocate(seq):
            break

        # è°ƒåº¦
        self.block_manager.allocate(seq)
        scheduled_seqs.append(seq)

    if scheduled_seqs:
        return scheduled_seqs, True  # Prefill æ¨¡å¼

    # é˜¶æ®µ2ï¼šDecodeï¼ˆç”Ÿæˆæ–° tokenï¼‰
    while self.running and num_seqs < self.max_num_seqs:
        seq = self.running.popleft()

        # æŠ¢å é€»è¾‘
        while not self.block_manager.can_append(seq):
            if self.running:
                self.preempt(self.running.pop())  # æŠ¢å æœ€é•¿çš„
            else:
                self.preempt(seq)
                break
        else:
            scheduled_seqs.append(seq)

    return scheduled_seqs, False  # Decode æ¨¡å¼
```

**ç‰¹ç‚¹**ï¼š
- âœ… ç®€å•ç›´è§‚
- âœ… ä¼˜å…ˆçŸ­è¯·æ±‚
- âŒ Prefill å’Œ Decode åˆ†ç¦»ï¼Œæ•ˆç‡æŸå¤±
- âŒ æ— æ³•å¤„ç†è¶…é•¿ prompt

### 5.2 mini-sglangï¼šChunked Prefill + æ··åˆè°ƒåº¦

**ä»£ç **ï¼š`scheduler/prefill.py`

```python
class PrefillManager:
    def __init__(self, cache_manager, table_manager, decode_manager):
        self.cache_manager = cache_manager
        self.table_manager = table_manager
        self.decode_manager = decode_manager

    def schedule(self, pending_reqs: List[Req], budget: int):
        """Chunked Prefill è°ƒåº¦"""
        ready_to_decode = []

        for req in pending_reqs:
            # æ£€æŸ¥ Radix Cache å‘½ä¸­
            cache_handle = self.cache_manager.query(req.input_ids)

            if cache_hit_length >= len(req.input_ids):
                # å®Œå…¨å‘½ä¸­ï¼Œç›´æ¥è¿›å…¥ Decode
                req.filling_status = "decode"
                ready_to_decode.append(req)
            else:
                # éƒ¨åˆ†å‘½ä¸­ï¼ŒChunked Prefill
                remaining_budget = budget - used_tokens
                if remaining_budget <= 0:
                    break  # é¢„ç®—ç”¨å®Œ

                # è®¡ç®—è¿™æ¬¡ prefill å¤šå°‘
                chunk_size = min(remaining_budget, max_prefill_length)
                new_chunk_end = cache_hit_length + chunk_size

                # åˆ›å»ºå­è¯·æ±‚ï¼ˆchunkï¼‰
                chunked_req = ChunkedReq(
                    parent=req,
                    chunk_start=cache_hit_length,
                    chunk_end=new_chunk_end
                )
                # æ‰§è¡Œ prefill...
```

**ç‰¹ç‚¹**ï¼š
- âœ… **Chunked Prefill**ï¼šé•¿ prompt åˆ†ç‰‡å¤„ç†
- âœ… **æ··åˆè°ƒåº¦**ï¼šPrefill å’Œ Decode å¯ä»¥åœ¨åŒä¸€æ‰¹æ¬¡
- âœ… **è‡ªé€‚åº”**ï¼šæ ¹æ®é¢„ç®—åŠ¨æ€è°ƒæ•´ chunk å¤§å°
- âŒ å¤æ‚åº¦é«˜

### 5.3 Overlap Schedulingï¼ˆmini-sglang ç‹¬æœ‰ï¼‰

**åŸç†**ï¼šCPU è°ƒåº¦ä¸ GPU è®¡ç®—é‡å 

```python
# scheduler.py:75-100
def _process_last_data(
    self, last_data: ForwardData | None, ongoing_data: ForwardData | None
) -> None:
    if last_data is None:
        return

    batch, (_, next_tokens_cpu, copy_done) = last_data[0].batch, last_data[1]
    copy_done.synchronize()  # ç­‰å¾… GPU å®Œæˆ

    # åœ¨ GPU è®¡ç®—çš„åŒæ—¶ï¼ŒCPU å¤„ç†ç»“æœ
    for i, req in enumerate(batch.reqs):
        next_token_id = next_tokens_cpu[i]
        req.append_host(next_token_id.unsqueeze(0))

        # å‡†å¤‡ä¸‹ä¸€è½®çš„å…ƒæ•°æ®ï¼ˆCPU å·¥ä½œï¼‰
        self.table_manager.update(req)
        ...
```

**æ—¶é—´çº¿å¯¹æ¯”**ï¼š

```
nano-vLLMï¼š
CPUè°ƒåº¦ â†’ GPUè®¡ç®— â†’ CPUå¤„ç† â†’ CPUè°ƒåº¦ â†’ GPUè®¡ç®— ...
â†‘ 2ms    â†‘ 8ms     â†‘ 1ms    â†‘ 2ms    â†‘ 8ms
æ€»å»¶è¿Ÿï¼š11ms/step

mini-sglang (Overlap)ï¼š
CPUè°ƒåº¦ â†’ GPUè®¡ç®—
           â†‘ 8ms           CPUå¤„ç† â†’ CPUè°ƒåº¦
                          â†‘ 1ms    â†‘ 2ms (ä¸GPUå¹¶è¡Œ)
æ€»å»¶è¿Ÿï¼š8ms/step (éšè—äº†3msçš„CPUå¼€é”€)
```

**æ€§èƒ½æå‡**ï¼š~20-30% (CPUå¯†é›†å‹åœºæ™¯)

---

## 6. æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”

### 6.1 Attention åç«¯

| æ¡†æ¶ | Prefill | Decode | å¤‡æ³¨ |
|------|---------|--------|------|
| **nano-vLLM** | FlashAttention2 | FlashAttn KV Cache | å•ä¸€åç«¯ |
| **mini-sglang** | FlashAttention2/3 | FlashInfer | å¯é…ç½®ä¸åŒåç«¯ |

**mini-sglang çš„çµæ´»æ€§**ï¼š

```bash
# Prefill ç”¨ FA3ï¼ŒDecode ç”¨ FlashInferï¼ˆH100 æœ€ä¼˜ï¼‰
python -m minisgl --model "Qwen3-32B" --attn fa,fi

# éƒ½ç”¨ FlashAttention2
python -m minisgl --model "Qwen3-32B" --attn fa

# éƒ½ç”¨ FlashInfer
python -m minisgl --model "Qwen3-32B" --attn fi
```

**åŸå› **ï¼š
- **FlashAttention3** (H100)ï¼šPrefill æå¿«ï¼Œä½† Decode ä¸€èˆ¬
- **FlashInfer**ï¼šDecode ä¼˜åŒ–å¥½ï¼ŒPrefill ä¹Ÿå¿«
- ç»„åˆä½¿ç”¨è¾¾åˆ°æœ€ä¼˜æ€§èƒ½

### 6.2 CUDA Graph

**nano-vLLM**ï¼šåŸºç¡€å®ç°

```python
# model_runner.py:216-252
def capture_cudagraph(self):
    for bs in [1, 2, 4, 8, 16, 32, ...]:
        graph = torch.cuda.CUDAGraph()
        outputs[:bs] = self.model(input_ids[:bs], positions[:bs])  # warmup
        with torch.cuda.graph(graph):
            outputs[:bs] = self.model(input_ids[:bs], positions[:bs])  # capture
        self.graphs[bs] = graph
```

**mini-sglang**ï¼šæ›´ç²¾ç»†çš„æ§åˆ¶

```python
# engine/graph.py
class CudaGraphRunner:
    def __init__(self, max_batch_size: int, capture_sizes: List[int]):
        self.max_batch_size = max_batch_size
        self.capture_sizes = capture_sizes

        # é¢„åˆ†é…æ›´å¤§çš„å†…å­˜æ± ï¼ˆæ”¯æŒåŠ¨æ€å¤§å°ï¼‰
        self.graph_pool = torch.cuda.graph_pool()

        # å¤šä¸ª graphï¼Œæ¯ä¸ªå¯¹åº”ä¸åŒçš„ batch size
        self.graphs = {}

    def replay(self, batch_size: int, *args):
        # é€‰æ‹©æœ€æ¥è¿‘çš„ graph
        graph_size = next(s for s in self.capture_sizes if s >= batch_size)
        graph = self.graphs[graph_size]

        # åªæ›´æ–°æœ‰æ•ˆéƒ¨åˆ†
        with torch.cuda.graph(graph):
            update_inputs(batch_size, args)
```

**å·®å¼‚**ï¼š
- **nano-vLLM**ï¼šå›ºå®šå¤§å°ï¼Œæµªè´¹å†…å­˜
- **mini-sglang**ï¼šåŠ¨æ€å¤§å°ï¼Œå†…å­˜æ•ˆç‡é«˜

### 6.3 è‡ªå®šä¹‰ CUDA Kernels

**nano-vLLM**ï¼šæ— è‡ªå®šä¹‰ kernelï¼Œå®Œå…¨ä¾èµ–ç¬¬ä¸‰æ–¹åº“

**mini-sglang**ï¼šåŒ…å«å¤šä¸ªä¼˜åŒ– kernel

```cpp
// kernel/csrc/jit/radix_cache.cu
// è‡ªå®šä¹‰ Radix Cache æ¯”è¾ƒ kernel
__global__ void fast_compare_key_kernel(
    const int* key,
    const int* input_ids,
    int* match_len,
    int key_len,
    int input_len
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= 1) return;

    int len = min(key_len, input_len);
    for (int i = 0; i < len; i++) {
        if (key[i] != input_ids[i]) {
            *match_len = i;
            return;
        }
    }
    *match_len = len;
}
```

**ä¼˜åŠ¿**ï¼š
- GPU åŸç”Ÿå®ç°ï¼Œé¿å… CPU-GPU ä¼ è¾“
- é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œæ¯”é€šç”¨å®ç°å¿« 2-5x

---

## 7. ä»£ç å¤æ‚åº¦å¯¹æ¯”

### 7.1 å­¦ä¹ æ›²çº¿

```
nano-vLLM:
â””â”€ å…¥é—¨æ—¶é—´ï¼š1-2 å¤©
   â”œâ”€ ç†è§£ PagedAttentionï¼š4 å°æ—¶
   â”œâ”€ ç†è§£ Schedulerï¼š2 å°æ—¶
   â”œâ”€ ç†è§£æ•´ä½“æµç¨‹ï¼š2 å°æ—¶
   â””â”€ å®éªŒä¿®æ”¹ï¼š1 å¤©

mini-sglang:
â””â”€ å…¥é—¨æ—¶é—´ï¼š1-2 å‘¨
   â”œâ”€ ç†è§£ Radix Cacheï¼š1 å¤©
   â”œâ”€ ç†è§£ Chunked Prefillï¼š1 å¤©
   â”œâ”€ ç†è§£ Overlap Schedulingï¼š1 å¤©
   â”œâ”€ ç†è§£å¤šè¿›ç¨‹é€šä¿¡ï¼š2 å¤©
   â”œâ”€ ç†è§£ CUDA Graphï¼š1 å¤©
   â””â”€ å®éªŒä¿®æ”¹ï¼š3-5 å¤©
```

### 7.2 å¯ç»´æŠ¤æ€§

| ç»´åº¦ | nano-vLLM | mini-sglang |
|------|-----------|-------------|
| **æ¨¡å—æ•°** | ~15 ä¸ª | ~40 ä¸ª |
| **ä¾èµ–æ•°** | ~10 ä¸ª | ~30 ä¸ª |
| **ä»£ç è¡Œæ•°** | ~2,000 | ~5,000 |
| **æ³¨é‡Šç‡** | ä½ | é«˜ |
| **ç±»å‹æ ‡æ³¨** | éƒ¨åˆ† | å®Œæ•´ |

### 7.3 æ‰©å±•æ€§

**nano-vLLM**ï¼š
```python
# æ·»åŠ æ–°æ¨¡å‹ç›¸å¯¹ç®€å•
# 1. å¤åˆ¶ models/qwen3.py
# 2. ä¿®æ”¹æ¨¡å‹ç»“æ„
# 3. å®ç° packed_modules_mapping
# 4. å®Œæˆï¼
```

**mini-sglang**ï¼š
```python
# æ·»åŠ æ–°æ¨¡å‹éœ€è¦è€ƒè™‘æ›´å¤š
# 1. å®ç° models/xxx.py
# 2. å®ç°ç‰¹å®šçš„ attention é…ç½®
# 3. å¤„ç†åˆ†å¸ƒå¼åŒæ­¥
# 4. æ›´æ–°æ–‡æ¡£å’Œæµ‹è¯•
# 5. æ€§èƒ½è°ƒä¼˜ï¼ˆCUDA kernelsï¼‰
```

---

## 8. é€‚ç”¨åœºæ™¯å¯¹æ¯”

### 8.1 nano-vLLM æœ€ä½³åœºæ™¯

âœ… **å¼ºçƒˆæ¨è**ï¼š

1. **å­¦ä¹  vLLM åŸç†**
   - ä»£ç ç®€æ´ï¼Œæ˜“äºç†è§£
   - æ ¸å¿ƒæ€æƒ³å®Œæ•´ä¿ç•™
   - é€‚åˆæ•™å­¦å’Œç ”ç©¶

2. **å¿«é€ŸåŸå‹å¼€å‘**
   - éªŒè¯æ–°ç®—æ³•
   - å®éªŒæ–°è°ƒåº¦ç­–ç•¥
   - æµ‹è¯•æ–°ä¼˜åŒ–æŠ€æœ¯

3. **å°è§„æ¨¡éƒ¨ç½²**
   - å• GPU æˆ–å°‘é‡ GPU
   - ä½å¹¶å‘åœºæ™¯
   - å¯¹å»¶è¿Ÿè¦æ±‚ä¸æç«¯

4. **è‡ªå®šä¹‰éœ€æ±‚**
   - éœ€è¦æ·±åº¦å®šåˆ¶
   - ä¸éœ€è¦å®Œæ•´ API æœåŠ¡
   - é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

âŒ **ä¸æ¨è**ï¼š

- ç”Ÿäº§ç¯å¢ƒé«˜å¹¶å‘æœåŠ¡
- éœ€è¦æµå¼è¾“å‡º
- å¤šç”¨æˆ·åœ¨çº¿æœåŠ¡
- è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ>32kï¼‰

### 8.2 mini-sglang æœ€ä½³åœºæ™¯

âœ… **å¼ºçƒˆæ¨è**ï¼š

1. **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**
   - OpenAI å…¼å®¹ API
   - é«˜å¹¶å‘åœ¨çº¿æœåŠ¡
   - å¤šç”¨æˆ· Chatbot

2. **é«˜æ€§èƒ½éœ€æ±‚**
   - å¯¹ååé‡è¦æ±‚é«˜
   - å¯¹å»¶è¿Ÿæ•æ„Ÿ
   - éœ€è¦æè‡´ä¼˜åŒ–

3. **å¤æ‚åœºæ™¯**
   - è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ>128kï¼‰
   - å¤šç”¨æˆ·å…±äº«ç³»ç»Ÿæç¤º
   - æ··åˆé•¿åº¦è¯·æ±‚

4. **åˆ†å¸ƒå¼éƒ¨ç½²**
   - å¤š GPU æœåŠ¡å™¨
   - éœ€è¦æ°´å¹³æ‰©å±•
   - éœ€è¦å®¹é”™å’Œç›‘æ§

âŒ **ä¸æ¨è**ï¼š

- å¿«é€Ÿå­¦ä¹ å’ŒåŸå‹ï¼ˆè¿‡äºå¤æ‚ï¼‰
- å•æœºä½å¹¶å‘ï¼ˆèµ„æºæµªè´¹ï¼‰
- é¢„ç®—å—é™ï¼ˆéœ€è¦å¤š GPUï¼‰

---

## 9. å­¦ä¹ è·¯å¾„å»ºè®®

### 9.1 åˆå­¦è€…è·¯å¾„ï¼ˆ1-2 å‘¨ï¼‰

```
Week 1: nano-vLLM æ·±åº¦å­¦ä¹ 
â”œâ”€ Day 1-2: ç†è§£æ•´ä½“æ¶æ„
â”‚   â””â”€ é˜…è¯» VLLM_TUTORIAL.md
â”œâ”€ Day 3-4: æ·±å…¥ PagedAttention
â”‚   â””â”€ ä¿®æ”¹ block_sizeï¼Œè§‚å¯Ÿæ•ˆæœ
â”œâ”€ Day 5-6: ç†è§£ Scheduler
â”‚   â””â”€ å®ç°ç®€å•çš„ FIFO è°ƒåº¦
â”œâ”€ Day 7: è¿è¡Œå’Œè°ƒè¯•
â”‚   â””â”€ ä½¿ç”¨ example.pyï¼Œæ·»åŠ æ—¥å¿—

Week 2: mini-sglang å¯¹æ¯”å­¦ä¹ 
â”œâ”€ Day 8-9: ç†è§£ Radix Cache
â”‚   â””â”€ å¯¹æ¯” BlockManager
â”œâ”€ Day 10-11: ç†è§£ Chunked Prefill
â”‚   â””â”€ æµ‹è¯•ä¸åŒ chunk size
â”œâ”€ Day 12-13: è¿è¡Œ benchmark
â”‚   â””â”€ å¯¹æ¯”ä¸¤ä¸ªæ¡†æ¶çš„æ€§èƒ½
â””â”€ Day 14: æ€»ç»“å’Œé¡¹ç›®å®è·µ
```

### 9.2 è¿›é˜¶è·¯å¾„ï¼ˆ1-2 ä¸ªæœˆï¼‰

```
Month 1: æ·±å…¥ä¼˜åŒ–
â”œâ”€ Week 1-2: æ€§èƒ½åˆ†æ
â”‚   â”œâ”€ ä½¿ç”¨ Nsight åˆ†æ GPU æ€§èƒ½
â”‚   â”œâ”€ æ‰¾åˆ°ç“¶é¢ˆå¹¶ä¼˜åŒ–
â”‚   â””â”€ å¯¹æ¯”ä¸åŒ attention åç«¯
â”œâ”€ Week 3-4: å®ç°æ–°ç‰¹æ€§
â”‚   â”œâ”€ Speculative Decoding
â”‚   â”œâ”€ Quantization (INT8/FP8)
â”‚   â””â”€ LoRA æ¨ç†æ”¯æŒ

Month 2: ç”Ÿäº§å®è·µ
â”œâ”€ Week 5-6: éƒ¨ç½²ä¼˜åŒ–
â”‚   â”œâ”€ Docker å®¹å™¨åŒ–
â”‚   â”œâ”€ Kubernetes ç¼–æ’
â”‚   â””â”€ ç›‘æ§å’Œæ—¥å¿—
â””â”€ Week 7-8: è‡ªå®šä¹‰å¼€å‘
    â”œâ”€ æ·»åŠ æ–°æ¨¡å‹æ”¯æŒ
    â”œâ”€ å®ç°è‡ªå®šä¹‰è°ƒåº¦ç­–ç•¥
    â””â”€ æ€§èƒ½è°ƒä¼˜å’Œ benchmark
```

### 9.3 å®æˆ˜é¡¹ç›®å»ºè®®

**é¡¹ç›®1ï¼šå¯¹æ¯”ä¸åŒç¼“å­˜ç­–ç•¥**

```python
# åœ¨ nano-vLLM ä¸­å®ç°ä¸‰ç§ç¼“å­˜
# 1. æ— ç¼“å­˜ï¼ˆbaselineï¼‰
# 2. ç®€å•å“ˆå¸Œç¼“å­˜ï¼ˆå½“å‰å®ç°ï¼‰
# 3. Radix Cacheï¼ˆå‚è€ƒ mini-sglangï¼‰

# æµ‹è¯•åœºæ™¯ï¼š
# - 100 ä¸ªè¯·æ±‚ï¼Œå‰ç¼€é•¿åº¦ 0-1000 éšæœº
# - æµ‹é‡ååé‡å’Œç¼“å­˜å‘½ä¸­ç‡
# - ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾
```

**é¡¹ç›®2ï¼šå®ç° Chunked Prefill**

```python
# åœ¨ nano-vLLM ä¸­æ·»åŠ  Chunked Prefill
# å‚è€ƒ mini-sglang çš„å®ç°

# å…³é”®ç‚¹ï¼š
# 1. åœ¨ Scheduler ä¸­æ·»åŠ  budget ç®¡ç†
# 2. å°†é•¿ prompt åˆ†æˆå¤šä¸ª chunk
# 3. æ¯ä¸ª Chunk ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ Sequence

# æµ‹è¯•ï¼š
# - 32k é•¿åº¦çš„ prompt
# - å¯¹æ¯”å®Œæ•´ prefill vs chunked prefill
# - æµ‹é‡å³°å€¼å†…å­˜å’Œååé‡
```

**é¡¹ç›®3ï¼šæ·»åŠ  Speculative Decoding**

```python
# åœ¨ä¸¤ä¸ªæ¡†æ¶ä¸­éƒ½å®ç° Speculative Decoding
# ä½¿ç”¨å°æ¨¡å‹ï¼ˆå¦‚ Qwen3-0.5Bï¼‰ä½œä¸º draft model

# å®ç°æ­¥éª¤ï¼š
# 1. draft model ç”Ÿæˆ k ä¸ªå€™é€‰ tokens
# 2. éªŒè¯æ¨¡å‹å¹¶è¡ŒéªŒè¯
# 3. æ¥å—/æ‹’ç»æœºåˆ¶

# å¯¹æ¯”ï¼š
# - ä¸åŒåœºæ™¯ä¸‹çš„åŠ é€Ÿæ¯”
# - ä¸åŒ draft model å¤§å°çš„å½±å“
```

---

## 10. æ€»ç»“

### 10.1 æ ¸å¿ƒå·®å¼‚æ€»ç»“

| ç»´åº¦ | nano-vLLM | mini-sglang |
|------|-----------|-------------|
| **å­¦ä¹ æ›²çº¿** | â­â­ ä½ | â­â­â­â­ é«˜ |
| **ä»£ç å¤æ‚åº¦** | â­â­ ä½ | â­â­â­â­â­ é«˜ |
| **æ€§èƒ½** | â­â­â­ å¥½ | â­â­â­â­â­ ä¼˜ç§€ |
| **æ‰©å±•æ€§** | â­â­ ä¸­ | â­â­â­â­â­ å¼º |
| **ç”Ÿäº§å°±ç»ª** | â­â­ ä¸­ | â­â­â­â­â­ å¼º |
| **ç¤¾åŒºæ”¯æŒ** | â­ å° | â­â­â­â­ å¤§ |
| **æ–‡æ¡£å®Œå–„åº¦** | â­â­ ä¸­ | â­â­â­â­ å¥½ |

### 10.2 é€‰æ‹©å»ºè®®

**é€‰æ‹© nano-vLLM å¦‚æœä½ **ï¼š
- æ­£åœ¨å­¦ä¹  LLM æ¨ç†ç³»ç»Ÿ
- éœ€è¦å¿«é€ŸéªŒè¯æƒ³æ³•
- éƒ¨ç½²è§„æ¨¡å°ï¼ˆå•æœº/å°‘ç”¨æˆ·ï¼‰
- å¸Œæœ›å®Œå…¨ç†è§£å’Œæ§åˆ¶ä»£ç 

**é€‰æ‹© mini-sglang å¦‚æœä½ **ï¼š
- éœ€è¦ç”Ÿäº§çº§éƒ¨ç½²
- è¿½æ±‚æè‡´æ€§èƒ½
- éœ€è¦å®Œæ•´ API æœåŠ¡
- æœ‰å¤š GPU èµ„æº

### 10.3 æœ€ä½³å®è·µ

**å­¦ä¹ è·¯å¾„**ï¼š
```
1. ä» nano-vLLM å¼€å§‹ï¼ˆ1-2å‘¨ï¼‰
   â””â”€ ç†è§£æ ¸å¿ƒæ¦‚å¿µ

2. å¯¹æ¯” mini-sglangï¼ˆ1-2å‘¨ï¼‰
   â””â”€ å­¦ä¹ é«˜çº§ä¼˜åŒ–

3. åŠ¨æ‰‹å®è·µï¼ˆæŒç»­ï¼‰
   â””â”€ å®ç°è‡ªå·±çš„ä¼˜åŒ–

4. ç”Ÿäº§éƒ¨ç½²ï¼ˆæŒ‰éœ€ï¼‰
   â””â”€ é€‰æ‹©åˆé€‚çš„æ¡†æ¶
```

**å¼€å‘å»ºè®®**ï¼š
- å…ˆç”¨ nano-vLLM éªŒè¯æƒ³æ³•
- å†åœ¨ mini-sglang ä¸­å®ç°ç”Ÿäº§ç‰ˆæœ¬
- å‚è€ƒä¸¤è€…ä»£ç ï¼Œå–é•¿è¡¥çŸ­

---

## 11. å‚è€ƒèµ„æº

### 11.1 è®ºæ–‡

- **PagedAttention**: "Efficient Memory Management for Large Language Model Serving with PagedAttention" (vLLM)
- **Radix Cache**: "Efficient LLM Inference with Radix Attention" (SGLang)
- **Chunked Prefill**: "Sarathi-Serve: Efficient LLM Serving over PCIe and NVLink Networks using Token-Chopping"
- **Overlap Scheduling**: "NanoFlow: A Microkernel-Based Inference System for Large Language Models"

### 11.2 é¡¹ç›®é“¾æ¥

- **nano-vLLM**: https://github.com/tzular/mini-vllm
- **mini-sglang**: https://github.com/sgl-project/mini-sglang
- **vLLM**: https://github.com/vllm-project/vllm
- **SGLang**: https://github.com/sgl-project/sglang

### 11.3 å·¥å…·

- **FlashAttention**: https://github.com/Dao-AILab/flash-attention
- **FlashInfer**: https://github.com/flashinfer-ai/flashinfer
- **Triton**: https://github.com/openai/triton

---

**ç¥å­¦ä¹ é¡ºåˆ©ï¼ğŸ‰**

æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿ç»§ç»­äº¤æµã€‚
