# nano-vLLM æ·±åº¦æ•™å­¦æŒ‡å—

> ä»é›¶ç†è§£ vLLMï¼šæ ¸å¿ƒåŸç†ã€ä»£ç å®ç°ä¸æ€§èƒ½ä¼˜åŒ–

---

## ç›®å½•

1. [ä¸ºä»€ä¹ˆè¦ç”¨ vLLMï¼Ÿ](#1-ä¸ºä»€ä¹ˆè¦ç”¨-vllm)
2. [æ ¸å¿ƒç»„ä»¶æ¦‚è§ˆ](#2-æ ¸å¿ƒç»„ä»¶æ¦‚è§ˆ)
3. [PagedAttention è¯¦è§£](#3-pagedattention-è¯¦è§£)
4. [Scheduler è°ƒåº¦å™¨](#4-scheduler-è°ƒåº¦å™¨)
5. [KV Cache ç®¡ç†](#5-kv-cache-ç®¡ç†)
6. [Continuous Batching](#6-continuous-batching)
7. [Prefix Caching](#7-prefix-caching)
8. [CUDA Graph ä¼˜åŒ–](#8-cuda-graph-ä¼˜åŒ–)
9. [å®æˆ˜ç»ƒä¹ ](#9-å®æˆ˜ç»ƒä¹ )

---

## 1. ä¸ºä»€ä¹ˆè¦ç”¨ vLLMï¼Ÿ

### 1.1 æ™®é€šæ¨ç†çš„ä¸‰å¤§ç—›ç‚¹

#### ç—›ç‚¹1ï¼šæ˜¾å­˜æµªè´¹ä¸¥é‡

```python
# æ™®é€šæ¨ç†ï¼šHuggingFace Transformers
prompt = "è§£é‡Šé‡å­è®¡ç®—çš„åŸç†"
output = model.generate(prompt, max_new_tokens=500)

# é—®é¢˜ï¼šéœ€è¦é¢„åˆ†é…æ•´ä¸ªåºåˆ—çš„ KV Cache
# å³ä½¿æˆ‘ä»¬åªéœ€è¦ 500 ä¸ªæ–° tokenï¼Œä¹Ÿè¦ä¸º 1000+ tokens é¢„ç•™ç©ºé—´
# å®é™…æ˜¾å­˜åˆ©ç”¨ç‡ï¼š~30%
```

**å›¾ç¤ºï¼šæ™®é€šæ¨ç†çš„æ˜¾å­˜åˆ†é…**

```
è¯·æ±‚1 (prompt=1000, ç”Ÿæˆ=1000)
â”œâ”€ KV Cache: é¢„åˆ†é… 2000 tokens
â”œâ”€ å®é™…ä½¿ç”¨: 2000 tokens âœ…
â””â”€ æµªè´¹: 0 tokens

è¯·æ±‚2 (prompt=100, ç”Ÿæˆ=100)
â”œâ”€ KV Cache: é¢„åˆ†é… 2000 tokens (å’Œè¯·æ±‚1ä¸€æ ·ï¼)
â”œâ”€ å®é™…ä½¿ç”¨: 200 tokens
â””â”€ æµªè´¹: 1800 tokens âŒ (90% æµªè´¹ï¼)
```

#### ç—›ç‚¹2ï¼šæ‰¹å¤„ç†æ•ˆç‡ä½

```python
# Static Batching: å¿…é¡»ç­‰å¾…æœ€æ…¢çš„è¯·æ±‚
batch = [
    "ç”Ÿæˆä¸€ç¯‡1000å­—çš„æ–‡ç« ",  # éœ€è¦5ç§’
    "ä½ å¥½",                 # éœ€è¦0.5ç§’
    "ä»€ä¹ˆæ˜¯AI",            # éœ€è¦1ç§’
]
# æ•´ä¸ªæ‰¹æ¬¡éœ€è¦5ç§’ï¼Œå³ä½¿å…¶ä»–è¯·æ±‚æ—©å®Œæˆäº†
# GPUåˆ©ç”¨ç‡ï¼š~40%
```

#### ç—›ç‚¹3ï¼šæ— æ³•æœ‰æ•ˆåˆ©ç”¨ç¼“å­˜

```python
# å¤šç”¨æˆ·ä½¿ç”¨ç›¸åŒçš„ç³»ç»Ÿæç¤ºè¯
system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š"

requests = [
    system_prompt + "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    system_prompt + "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    system_prompt + "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ",
]

# æ™®é€šæ¨ç†ï¼šæ¯ä¸ªè¯·æ±‚éƒ½è¦é‡æ–°è®¡ç®— system_prompt çš„ KV Cache
# æµªè´¹äº†å¤§é‡è®¡ç®—ï¼
```

### 1.2 vLLM çš„è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | æ™®é€šæ¨ç† | vLLM | æå‡ |
|------|---------|------|------|
| æ˜¾å­˜åˆ©ç”¨ç‡ | ~30% | **>90%** | 3x |
| ååé‡ | 50 tok/s | **2000 tok/s** | 40x |
| å‰ç¼€ç¼“å­˜ | ä¸æ”¯æŒ | **10-100x åŠ é€Ÿ** | - |
| å¹¶å‘èƒ½åŠ› | 8 è¯·æ±‚ | **256+ è¯·æ±‚** | 32x |

---

## 2. æ ¸å¿ƒç»„ä»¶æ¦‚è§ˆ

### 2.1 æ¶æ„å›¾

```
ç”¨æˆ·è¯·æ±‚
    â†“
LLM.generate()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LLMEngine (ä¸»å¼•æ“)             â”‚
â”‚  - ç®¡ç†è¯·æ±‚é˜Ÿåˆ—                           â”‚
â”‚  - åè°ƒå„ç»„ä»¶                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Scheduler (è°ƒåº¦å™¨)              â”‚
â”‚  - å†³å®šå“ªäº›è¯·æ±‚å¯ä»¥æ‰§è¡Œ                   â”‚
â”‚  - æŠ¢å å¼è°ƒåº¦                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       BlockManager (å—ç®¡ç†å™¨)            â”‚
â”‚  - åˆ†é…/é‡Šæ”¾ KV Cache å—                 â”‚
â”‚  - å‰ç¼€ç¼“å­˜æŸ¥æ‰¾                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ModelRunner (æ¨¡å‹æ‰§è¡Œ)            â”‚
â”‚  - Prefill / Decode æ‰§è¡Œ                 â”‚
â”‚  - CUDA Graph ä¼˜åŒ–                       â”‚
â”‚  - Tensor Parallelism                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
GPU è®¡ç®—
```

### 2.2 æ•°æ®æµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt  â”‚ 1. ç”¨æˆ·è¾“å…¥
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Sequence â”‚ 2. å°è£…æˆåºåˆ—å¯¹è±¡
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduler    â”‚ 3. åŠ å…¥ç­‰å¾…é˜Ÿåˆ—
â”‚   .waiting   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduler    â”‚ 4. è°ƒåº¦å†³ç­–
â”‚   .schedule()â”‚    - æœ‰èµ„æºå—ï¼Ÿ
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    - æŠ¢å ï¼Ÿ
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚BlockManager  â”‚ 5. åˆ†é… KV Cache å—
â”‚  .allocate() â”‚    - æŸ¥æ‰¾å‰ç¼€ç¼“å­˜
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    - åˆ†é…æ–°å—
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ModelRunner   â”‚ 6. æ‰§è¡Œæ¨¡å‹
â”‚    .run()    â”‚    - Prefill æˆ– Decode
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    - é‡‡æ ·ç”Ÿæˆ
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduler    â”‚ 7. æ›´æ–°çŠ¶æ€
â”‚.postprocess()â”‚    - æ·»åŠ æ–° token
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    - æ£€æŸ¥æ˜¯å¦å®Œæˆ
```

---

## 3. PagedAttention è¯¦è§£

### 3.1 æ ¸å¿ƒæ€æƒ³ï¼šå€Ÿé‰´æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜

**ä¼ ç»Ÿæ–¹å¼**ï¼šè¿ç»­åˆ†é…
```
è¯·æ±‚A: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 1000 tokensï¼Œè¿ç»­å†…å­˜
è¯·æ±‚B: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]              100 tokensï¼Œè¿ç»­å†…å­˜
       â†‘ æµªè´¹ï¼šéœ€è¦é¢„åˆ†é…æ•´ä¸ªåºåˆ—ç©ºé—´
```

**PagedAttention**ï¼šåˆ†é¡µç®¡ç†
```
è¯·æ±‚A: [â–ˆâ–ˆ][â–ˆâ–ˆ][â–ˆâ–ˆ][â–ˆâ–ˆ] 4ä¸ªå—ï¼Œæ¯å—256 tokens
è¯·æ±‚B: [â–ˆâ–ˆ]             1ä¸ªå—ï¼Œ256 tokensï¼ˆåªç”¨100ï¼‰
       â†‘ æŒ‰éœ€åˆ†é…ï¼Œä¸æµªè´¹
```

### 3.2 ä»£ç å®ç°ï¼šBlock ç±»

**æ–‡ä»¶**: `nanovllm/engine/block_manager.py:8-24`

```python
class Block:
    """KV Cache çš„ä¸€ä¸ªç‰©ç†å—"""

    def __init__(self, block_id):
        self.block_id = block_id        # å—çš„å”¯ä¸€æ ‡è¯†
        self.ref_count = 0              # å¼•ç”¨è®¡æ•°ï¼ˆæ”¯æŒå…±äº«ï¼‰
        self.hash = -1                  # å—å†…å®¹çš„å“ˆå¸Œå€¼
        self.token_ids = []             # å—å†…çš„ token åºåˆ—

    def update(self, hash: int, token_ids: list[int]):
        """æ›´æ–°å—å†…å®¹"""
        self.hash = hash
        self.token_ids = token_ids

    def reset(self):
        """é‡ç½®å—çŠ¶æ€ï¼ˆç”¨äºå¤ç”¨ï¼‰"""
        self.ref_count = 1
        self.hash = -1
        self.token_ids = []
```

**å…³é”®æ¦‚å¿µ**ï¼š
- `block_id`: ç‰©ç†å—çš„åœ°å€ï¼ˆç±»ä¼¼å†…å­˜é¡µå·ï¼‰
- `ref_count`: å¤šä¸ªåºåˆ—å¯ä»¥å…±äº«åŒä¸€ä¸ªå—ï¼ˆå‰ç¼€ç¼“å­˜ï¼‰
- `hash`: ç”¨äºå¿«é€ŸæŸ¥æ‰¾ç›¸åŒå†…å®¹çš„å—

### 3.3 Sequence çš„å—è¡¨

**æ–‡ä»¶**: `nanovllm/engine/sequence.py:14-84`

```python
class Sequence:
    block_size = 256  # æ¯ä¸ªå—çš„å¤§å°

    def __init__(self, token_ids: list[int], sampling_params):
        self.token_ids = token_ids       # å®Œæ•´çš„ token åºåˆ—
        self.block_table = []            # é€»è¾‘å— â†’ ç‰©ç†å—çš„æ˜ å°„è¡¨
        self.num_cached_tokens = 0       # å‘½ä¸­å‰ç¼€ç¼“å­˜çš„ token æ•°

    @property
    def num_blocks(self):
        """éœ€è¦å¤šå°‘ä¸ªå—"""
        return (self.num_tokens + self.block_size - 1) // self.block_size

    def block(self, i):
        """è·å–ç¬¬ i ä¸ªé€»è¾‘å—çš„ token åˆ—è¡¨"""
        return self.token_ids[i*self.block_size: (i+1)*self.block_size]
```

**ç¤ºä¾‹ï¼š1000 tokens çš„åºåˆ—**

```python
seq = Sequence([1, 2, 3, ..., 1000])  # 1000 ä¸ª tokens

# éœ€è¦å¤šå°‘ä¸ªå—ï¼Ÿ
print(seq.num_blocks)  # 4 ä¸ªå—
# [0-255], [256-511], [512-767], [768-999]

# è®¿é—®ç¬¬ 2 ä¸ªå—
print(seq.block(1))  # [256, 257, ..., 511]
```

### 3.4 BlockManagerï¼šåˆ†é…ä¸é‡Šæ”¾

**æ–‡ä»¶**: `nanovllm/engine/block_manager.py:26-113`

#### 3.4.1 åˆå§‹åŒ–

```python
class BlockManager:
    def __init__(self, num_blocks: int, block_size: int):
        self.block_size = block_size
        self.blocks = [Block(i) for i in range(num_blocks)]  # æ‰€æœ‰ç‰©ç†å—
        self.hash_to_block_id = {}                            # å“ˆå¸Œ â†’ å—ID æ˜ å°„
        self.free_block_ids = deque(range(num_blocks))        # ç©ºé—²å—é˜Ÿåˆ—
        self.used_block_ids = set()                           # å·²ç”¨å—é›†åˆ
```

**ç¤ºä¾‹**ï¼š
```python
# å‡è®¾æœ‰ 1000 ä¸ªå—ï¼Œæ¯å— 256 tokens
manager = BlockManager(num_blocks=1000, block_size=256)

print(len(manager.free_block_ids))  # 1000 ä¸ªç©ºé—²å—
print(len(manager.used_block_ids))  # 0 ä¸ªå·²ç”¨å—
```

#### 3.4.2 åˆ†é…å—

```python
def allocate(self, seq: Sequence):
    """ä¸ºåºåˆ—åˆ†é…ç‰©ç†å—"""
    h = -1  # æ»šåŠ¨å“ˆå¸Œ
    cache_miss = False

    for i in range(seq.num_blocks):
        token_ids = seq.block(i)

        # è®¡ç®—å—çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºå‰ç¼€ç¼“å­˜æŸ¥æ‰¾ï¼‰
        if len(token_ids) == self.block_size:
            h = self.compute_hash(token_ids, h)
        else:
            h = -1  # ä¸å®Œæ•´çš„å—ä¸ç¼“å­˜

        # æŸ¥æ‰¾æ˜¯å¦å·²æœ‰ç›¸åŒå†…å®¹çš„å—
        block_id = self.hash_to_block_id.get(h, -1)

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦çœŸæ­£å‘½ä¸­ï¼ˆé¿å…å“ˆå¸Œå†²çªï¼‰
        if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
            cache_miss = True

        if cache_miss:
            # ç¼“å­˜æœªå‘½ä¸­ï¼šåˆ†é…æ–°å—
            block_id = self.free_block_ids[0]
            block = self._allocate_block(block_id)
        else:
            # ç¼“å­˜å‘½ä¸­ï¼šå¤ç”¨å·²æœ‰å—ï¼
            seq.num_cached_tokens += self.block_size
            block = self.blocks[block_id]
            block.ref_count += 1  # å¢åŠ å¼•ç”¨è®¡æ•°

        # æ›´æ–°å“ˆå¸Œè¡¨
        if h != -1:
            block.update(h, token_ids)
            self.hash_to_block_id[h] = block_id

        # è®°å½•æ˜ å°„å…³ç³»ï¼šé€»è¾‘å— i â†’ ç‰©ç†å— block_id
        seq.block_table.append(block_id)
```

**ç¤ºä¾‹ï¼šåˆ†é…è¿‡ç¨‹**

```python
# åºåˆ— Aï¼š1000 tokens
seq_a = Sequence(list(range(1000)))
manager.allocate(seq_a)

# åˆ†é…ç»“æœï¼š
# seq_a.block_table = [0, 1, 2, 3]
# é€»è¾‘å— 0 â†’ ç‰©ç†å— 0 (tokens 0-255)
# é€»è¾‘å— 1 â†’ ç‰©ç†å— 1 (tokens 256-511)
# é€»è¾‘å— 2 â†’ ç‰©ç†å— 2 (tokens 512-767)
# é€»è¾‘å— 3 â†’ ç‰©ç†å— 3 (tokens 768-999)

print(len(manager.free_block_ids))  # 996 (1000 - 4)
```

#### 3.4.3 è¿½åŠ æ–° tokenï¼ˆDecode é˜¶æ®µï¼‰

```python
def may_append(self, seq: Sequence):
    """ä¸ºåºåˆ—è¿½åŠ ä¸€ä¸ªæ–° token"""
    block_table = seq.block_table
    last_block = self.blocks[block_table[-1]]

    current_len = len(seq)
    if current_len % self.block_size == 1:
        # å½“å‰ token æ˜¯æ–°å—çš„ç¬¬ä¸€ä¸ª
        # éœ€è¦åˆ†é…æ–°å—
        assert last_block.hash != -1
        block_id = self.free_block_ids[0]
        self._allocate_block(block_id)
        block_table.append(block_id)

    elif current_len % self.block_size == 0:
        # å½“å‰ token å¡«æ»¡äº†æœ€åä¸€ä¸ªå—
        # è®¡ç®—å“ˆå¸Œå¹¶åŠ å…¥ç¼“å­˜
        assert last_block.hash == -1
        token_ids = seq.block(seq.num_blocks - 1)
        prefix = self.blocks[block_table[-2]].hash if len(block_table) > 1 else -1
        h = self.compute_hash(token_ids, prefix)
        last_block.update(h, token_ids)
        self.hash_to_block_id[h] = last_block.block_id
```

**ç¤ºä¾‹ï¼šåºåˆ—å¢é•¿**

```python
# åˆå§‹ï¼š1000 tokens (4ä¸ªå—)
seq = Sequence(list(range(1000)))
manager.allocate(seq)
print(seq.block_table)  # [0, 1, 2, 3]

# ç”Ÿæˆ 1 ä¸ªæ–° tokenï¼š1001
seq.append_token(1000)
manager.may_append(seq)
print(seq.block_table)  # [0, 1, 2, 3, 4] â† æ–°å¢ç¬¬ 5 ä¸ªå—

# ç»§ç»­ç”Ÿæˆ 254 ä¸ª tokenï¼Œå¡«æ»¡ç¬¬ 5 ä¸ªå—
for i in range(1001, 1256):
    seq.append_token(i)
    manager.may_append(seq)
print(seq.block_table)  # [0, 1, 2, 3, 4]
# ç¬¬ 4 ä¸ªå—ï¼ˆç´¢å¼• 4ï¼‰ç°åœ¨è¢«ç¼“å­˜äº†
```

#### 3.4.4 é‡Šæ”¾å—

```python
def deallocate(self, seq: Sequence):
    """é‡Šæ”¾åºåˆ—å ç”¨çš„æ‰€æœ‰å—"""
    for block_id in reversed(seq.block_table):
        block = self.blocks[block_id]
        block.ref_count -= 1  # å‡å°‘å¼•ç”¨è®¡æ•°
        if block.ref_count == 0:
            # å¼•ç”¨è®¡æ•°ä¸º 0ï¼ŒçœŸæ­£é‡Šæ”¾
            self._deallocate_block(block_id)

    seq.num_cached_tokens = 0
    seq.block_table.clear()
```

**ç¤ºä¾‹ï¼šé‡Šæ”¾ä¸å…±äº«**

```python
# åºåˆ— Aï¼š1000 tokens
seq_a = Sequence(list(range(1000)))
manager.allocate(seq_a)
# seq_a.block_table = [0, 1, 2, 3]

# åºåˆ— Bï¼šå‰ 500 tokens å’Œ A ç›¸åŒ
seq_b = Sequence(list(range(500)) + [9999]*500)
manager.allocate(seq_b)
# seq_b.block_table = [0, 1, 2, 4]
# â†‘ å‰ 2 ä¸ªå—ï¼ˆ0, 1ï¼‰å’Œ A å…±äº«ï¼

print(manager.blocks[0].ref_count)  # 2 (è¢« A å’Œ B å¼•ç”¨)
print(manager.blocks[2].ref_count)  # 1 (åªè¢« B å¼•ç”¨)

# é‡Šæ”¾åºåˆ— A
manager.deallocate(seq_a)
print(manager.blocks[0].ref_count)  # 1 (ä»ç„¶è¢« B å¼•ç”¨)
print(manager.blocks[2].ref_count)  # 0 (è¢«é‡Šæ”¾)
```

### 3.5 PagedAttention çš„ä¼˜åŠ¿

**1. æ˜¾å­˜åˆ©ç”¨ç‡**
```
ä¼ ç»Ÿæ–¹å¼ï¼š
- æ¯ä¸ªåºåˆ—é¢„åˆ†é…æœ€å¤§é•¿åº¦ï¼ˆå¦‚ 2000 tokensï¼‰
- çŸ­åºåˆ—æµªè´¹å¤§é‡æ˜¾å­˜

PagedAttentionï¼š
- æŒ‰éœ€åˆ†é…ï¼Œ256 tokens ä¸€ä¸ªå—
- æ˜¾å­˜åˆ©ç”¨ç‡ > 90%
```

**2. æ”¯æŒå‰ç¼€ç¼“å­˜**
```
å¤šä¸ªåºåˆ—å…±äº«ç›¸åŒçš„ prompt å—
èŠ‚çœæ˜¾å­˜ + è®¡ç®—æ—¶é—´
```

**3. çµæ´»çš„åºåˆ—é•¿åº¦**
```
åºåˆ—å¯ä»¥åŠ¨æ€å¢é•¿ï¼Œä¸éœ€è¦é¢„åˆ†é…
æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡
```

---

## 4. Scheduler è°ƒåº¦å™¨

### 4.1 è°ƒåº¦å™¨çš„èŒè´£

Scheduler æ˜¯ vLLM çš„"å¤§è„‘"ï¼Œè´Ÿè´£ï¼š
1. å†³å®šå“ªäº›è¯·æ±‚å¯ä»¥æ‰§è¡Œ
2. èµ„æºä¸è¶³æ—¶æŠ¢å æŸäº›è¯·æ±‚
3. åŒºåˆ† Prefill å’Œ Decode ä¸¤ä¸ªé˜¶æ®µ

### 4.2 æ ¸å¿ƒæ•°æ®ç»“æ„

**æ–‡ä»¶**: `nanovllm/engine/scheduler.py:8-17`

```python
class Scheduler:
    def __init__(self, config: Config):
        self.max_num_seqs = config.max_num_seqs                    # æœ€å¤§å¹¶å‘åºåˆ—æ•°
        self.max_num_batched_tokens = config.max_num_batched_tokens  # æœ€å¤§ batch token æ•°
        self.eos = config.eos                                      # ç»“æŸ token

        # BlockManagerï¼šç®¡ç† KV Cache å—
        self.block_manager = BlockManager(
            config.num_kvcache_blocks,
            config.kvcache_block_size
        )

        # ä¸‰ä¸ªé˜Ÿåˆ—
        self.waiting: deque[Sequence] = deque()  # ç­‰å¾…é˜Ÿåˆ—
        self.running: deque[Sequence] = deque()  # è¿è¡Œé˜Ÿåˆ—
```

**åºåˆ—çŠ¶æ€æœº**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WAITING â”‚  åœ¨ç­‰å¾…é˜Ÿåˆ—ï¼Œå°šæœªåˆ†é…èµ„æº
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ schedule() é€‰æ‹©æ‰§è¡Œ
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RUNNING â”‚  æ­£åœ¨ç”Ÿæˆï¼Œå·²åˆ†é… KV Cache
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ ç”Ÿæˆå®Œæˆ æˆ– è¾¾åˆ° max_tokens
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FINISHED â”‚  å®Œæˆï¼Œé‡Šæ”¾èµ„æº
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 è°ƒåº¦ç­–ç•¥

**æ–‡ä»¶**: `nanovllm/engine/scheduler.py:24-58`

#### 4.3.1 é˜¶æ®µ1ï¼šPrefillï¼ˆå¤„ç†æ–°è¯·æ±‚ï¼‰

```python
def schedule(self) -> tuple[list[Sequence], bool]:
    scheduled_seqs = []
    num_seqs = 0
    num_batched_tokens = 0

    # å°è¯•ä»ç­‰å¾…é˜Ÿåˆ—å–è¯·æ±‚
    while self.waiting and num_seqs < self.max_num_seqs:
        seq = self.waiting[0]

        # æ£€æŸ¥èµ„æºæ˜¯å¦è¶³å¤Ÿ
        if num_batched_tokens + len(seq) > self.max_num_batched_tokens:
            # è¶…è¿‡æœ€å¤§ batch token æ•°
            break

        if not self.block_manager.can_allocate(seq):
            # æ²¡æœ‰è¶³å¤Ÿçš„ KV Cache å—
            break

        # èµ„æºè¶³å¤Ÿï¼Œåˆ†é…å¹¶è¿è¡Œ
        num_seqs += 1
        self.block_manager.allocate(seq)
        num_batched_tokens += len(seq) - seq.num_cached_tokens  # å‡å»ç¼“å­˜å‘½ä¸­çš„

        seq.status = SequenceStatus.RUNNING
        self.waiting.popleft()
        self.running.append(seq)
        scheduled_seqs.append(seq)

    if scheduled_seqs:
        return scheduled_seqs, True  # Prefill æ¨¡å¼
```

**ç¤ºä¾‹ï¼šPrefill è°ƒåº¦**

```python
# é…ç½®
max_num_seqs = 4
max_num_batched_tokens = 4096

# ç­‰å¾…é˜Ÿåˆ—ï¼ˆåºåˆ—é•¿åº¦ï¼štokensï¼‰
waiting = [
    seq_a,  # 1000 tokens
    seq_b,  # 2000 tokens
    seq_c,  # 500 tokens
    seq_d,  # 3000 tokens
]

# ç¬¬1è½®è°ƒåº¦
# å°è¯• seq_a: 1000 < 4096 âœ…
# å°è¯• seq_b: 1000 + 2000 = 3000 < 4096 âœ…
# å°è¯• seq_c: 3000 + 500 = 3500 < 4096 âœ…
# å°è¯• seq_d: 3500 + 3000 = 6500 > 4096 âŒ åœæ­¢

scheduled = [seq_a, seq_b, seq_c]  # è°ƒåº¦3ä¸ªè¯·æ±‚
```

#### 4.3.2 é˜¶æ®µ2ï¼šDecodeï¼ˆç”Ÿæˆæ–° tokenï¼‰

```python
    # å¦‚æœæ²¡æœ‰æ–°è¯·æ±‚ï¼Œå¤„ç†æ­£åœ¨è¿è¡Œçš„åºåˆ—
    while self.running and num_seqs < self.max_num_seqs:
        seq = self.running.popleft()

        # æ£€æŸ¥èƒ½å¦è¿½åŠ æ–° token
        while not self.block_manager.can_append(seq):
            # èµ„æºä¸è¶³ï¼Œéœ€è¦æŠ¢å 
            if self.running:
                # æŠ¢å è¿è¡Œé˜Ÿåˆ—çš„æœ€åä¸€ä¸ªåºåˆ—ï¼ˆé€šå¸¸æ˜¯æœ€é•¿çš„ï¼‰
                self.preempt(self.running.pop())
            else:
                # æ²¡æœ‰å…¶ä»–å¯æŠ¢å çš„ï¼ŒæŠ¢å è‡ªå·±
                self.preempt(seq)
                break
        else:
            # èµ„æºè¶³å¤Ÿï¼Œç»§ç»­è¿è¡Œ
            num_seqs += 1
            self.block_manager.may_append(seq)
            scheduled_seqs.append(seq)

    assert scheduled_seqs
    self.running.extendleft(reversed(scheduled_seqs))
    return scheduled_seqs, False  # Decode æ¨¡å¼
```

**ç¤ºä¾‹ï¼šDecode æŠ¢å **

```python
# è¿è¡Œé˜Ÿåˆ—
running = [
    seq_a,  # å·²ç”Ÿæˆ 5000 tokensï¼Œblock_table = [0, 1, ..., 19]
    seq_b,  # å·²ç”Ÿæˆ 1000 tokensï¼Œblock_table = [20, 21, 22, 23]
]

# å‡è®¾åªå‰© 1 ä¸ªç©ºé—²å—

# å¤„ç† seq_aï¼šå¯ä»¥è¿½åŠ  âœ…
# å¤„ç† seq_bï¼šæ— æ³•è¿½åŠ ï¼ˆéœ€è¦æ–°å—ä½†æ²¡æœ‰ç©ºé—²ï¼‰

# æŠ¢å  seq_aï¼ˆæœ€é•¿çš„ï¼‰
preempt(seq_a)
# seq_a å›åˆ°ç­‰å¾…é˜Ÿåˆ—ï¼Œé‡Šæ”¾ 20 ä¸ªå—

# ç°åœ¨ seq_b å¯ä»¥ç»§ç»­äº†
```

### 4.4 æŠ¢å æœºåˆ¶

**æ–‡ä»¶**: `nanovllm/engine/scheduler.py:60-63`

```python
def preempt(self, seq: Sequence):
    """æŠ¢å åºåˆ—ï¼Œé‡Šæ”¾èµ„æº"""
    seq.status = SequenceStatus.WAITING
    self.block_manager.deallocate(seq)  # é‡Šæ”¾æ‰€æœ‰ KV Cache å—
    self.waiting.appendleft(seq)         # æ”¾å›ç­‰å¾…é˜Ÿåˆ—å¤´éƒ¨
```

**æŠ¢å ç­–ç•¥**ï¼š
- ä¼˜å…ˆæŠ¢å æœ€é•¿çš„åºåˆ—ï¼ˆå ç”¨èµ„æºå¤šï¼‰
- è¢«æŠ¢å çš„åºåˆ—ä¸‹æ¬¡é‡æ–° Prefill
- ç‰ºç‰²**å•ä¸ª**å»¶è¿Ÿï¼Œæå‡**æ•´ä½“**åå

**ç¤ºä¾‹ï¼šæŠ¢å çš„æ•ˆæœ**

```python
# åœºæ™¯ï¼š256 ä¸ªå¹¶å‘è¯·æ±‚

# æ— æŠ¢å ï¼ˆFIFOï¼‰ï¼š
# é•¿è¯·æ±‚é˜»å¡çŸ­è¯·æ±‚
# å¹³å‡å»¶è¿Ÿï¼š50 ç§’

# æœ‰æŠ¢å ï¼š
# çŸ­è¯·æ±‚ä¼˜å…ˆå®Œæˆ
# é•¿è¯·æ±‚è¢«æŠ¢å å¤šæ¬¡
# å¹³å‡å»¶è¿Ÿï¼š10 ç§’ï¼ˆçŸ­ï¼‰+ 100 ç§’ï¼ˆé•¿ï¼‰
# P99 å»¶è¿Ÿå¤§å¹…é™ä½
```

### 4.5 åå¤„ç†

**æ–‡ä»¶**: `nanovllm/engine/scheduler.py:65-72`

```python
def postprocess(self, seqs: list[Sequence], token_ids: list[int]):
    """å¤„ç†ç”Ÿæˆçš„ token"""
    for seq, token_id in zip(seqs, token_ids):
        seq.append_token(token_id)

        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        if (not seq.ignore_eos and token_id == self.eos) or \
           seq.num_completion_tokens == seq.max_tokens:
            # å®Œæˆï¼šé‡Šæ”¾èµ„æº
            seq.status = SequenceStatus.FINISHED
            self.block_manager.deallocate(seq)
            self.running.remove(seq)
```

### 4.6 è°ƒåº¦æµç¨‹æ€»ç»“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. æ£€æŸ¥ waiting é˜Ÿåˆ—               â”‚
â”‚     â””â”€> èƒ½å¦ Prefill æ–°è¯·æ±‚ï¼Ÿ        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ å¯ä»¥
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. åˆ†é… KV Cache å—                â”‚
â”‚     â””â”€> æŸ¥æ‰¾å‰ç¼€ç¼“å­˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. æ‰§è¡Œ Prefill                     â”‚
â”‚     â””â”€> ç”Ÿæˆç¬¬ä¸€ä¸ª completion token  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. æ£€æŸ¥ running é˜Ÿåˆ—               â”‚
â”‚     â””â”€> èƒ½å¦ Decodeï¼Ÿ                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ èµ„æºä¸è¶³
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. æŠ¢å æœ€é•¿çš„åºåˆ—                   â”‚
â”‚     â””â”€> é‡Šæ”¾å…¶ KV Cache              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. æ‰§è¡Œ Decode                     â”‚
â”‚     â””â”€> ç”Ÿæˆ 1 ä¸ª token             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. åå¤„ç†                          â”‚
â”‚     â””â”€> æ£€æŸ¥æ˜¯å¦å®Œæˆ                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. KV Cache ç®¡ç†

### 5.1 ä»€ä¹ˆæ˜¯ KV Cacheï¼Ÿ

åœ¨ Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼š
- **K (Key)**: ç”¨äºè®¡ç®—æ³¨æ„åŠ›æƒé‡
- **V (Value)**: æ ¹æ®æ³¨æ„åŠ›æƒé‡èšåˆä¿¡æ¯

```python
# ä¼ ç»Ÿæ–¹å¼ï¼šæ¯æ¬¡éƒ½é‡æ–°è®¡ç®—
for step in range(max_tokens):
    output = model(input_ids + generated_ids)
    # é‡å¤è®¡ç®—äº†ä¹‹å‰æ‰€æœ‰ä½ç½®çš„ K å’Œ Vï¼

# KV Cacheï¼šåªè®¡ç®—æ–°çš„ä½ç½®
for step in range(max_tokens):
    k, v = model.compute_kv(new_token)  # åªè®¡ç®—æ–°çš„
    k_cache.append(k)
    v_cache.append(v)
    output = modelAttention(k_cache, v_cache)  # ä½¿ç”¨ç¼“å­˜
```

### 5.2 KV Cache çš„å­˜å‚¨æ ¼å¼

**æ–‡ä»¶**: `nanovllm/engine/model_runner.py:100-118`

```python
def allocate_kv_cache(self):
    """åˆ†é… KV Cache æ˜¾å­˜"""
    config = self.config
    hf_config = config.hf_config

    # è®¡ç®—å¯ç”¨æ˜¾å­˜
    free, total = torch.cuda.mem_get_info()
    used = total - free
    peak = torch.cuda.memory_stats()["allocated_bytes.all.peak"]
    current = torch.cuda.memory_stats()["allocated_bytes.all.current"]

    # è®¡ç®—ä¸€ä¸ªå—çš„å¤§å°ï¼ˆå­—èŠ‚æ•°ï¼‰
    num_kv_heads = hf_config.num_key_value_heads // self.world_size
    head_dim = hf_config.hidden_size // hf_config.num_attention_heads
    block_bytes = (
        2 *                              # K å’Œ V
        hf_config.num_hidden_layers *   # å±‚æ•°
        self.block_size *               # æ¯å— token æ•°
        num_kv_heads *                  # KV å¤´æ•°
        head_dim *                      # å¤´ç»´åº¦
        hf_config.torch_dtype.itemsize  # æ•°æ®ç±»å‹å¤§å°
    )

    # è®¡ç®—å¯ä»¥åˆ†é…å¤šå°‘ä¸ªå—
    config.num_kvcache_blocks = int(
        total * config.gpu_memory_utilization - used - peak + current
    ) // block_bytes

    # åˆ†é… KV Cache å¼ é‡
    # shape: [2, num_layers, num_blocks, block_size, num_kv_heads, head_dim]
    self.kv_cache = torch.empty(
        2,  # K å’Œ V
        hf_config.num_hidden_layers,
        config.num_kvcache_blocks,
        self.block_size,
        num_kv_heads,
        head_dim
    )

    # å°†æ¯å±‚çš„ KV Cache æŒ‡é’ˆèµ‹å€¼ç»™ Attention å±‚
    layer_id = 0
    for module in self.model.modules():
        if hasattr(module, "k_cache") and hasattr(module, "v_cache"):
            module.k_cache = self.kv_cache[0, layer_id]
            module.v_cache = self.kv_cache[1, layer_id]
            layer_id += 1
```

**ç¤ºä¾‹ï¼šKV Cache å¤§å°è®¡ç®—**

```python
# é…ç½®
num_layers = 32
block_size = 256
num_kv_heads = 8
head_dim = 128
dtype = torch.float16  # 2 bytes

# ä¸€ä¸ªå—çš„å¤§å°
block_bytes = 2 * 32 * 256 * 8 * 128 * 2
            = 8,388,608 bytes â‰ˆ 8 MB

# 1000 ä¸ªå—
total_bytes = 1000 * 8 MB = 8 GB
```

### 5.3 slot_mappingï¼šé€»è¾‘ä½ç½®åˆ°ç‰©ç†ä½ç½®çš„æ˜ å°„

**é—®é¢˜**ï¼šåºåˆ—çš„ token åœ¨é€»è¾‘ä¸Šæ˜¯è¿ç»­çš„ï¼Œä½†ç‰©ç†å­˜å‚¨å¯èƒ½åˆ†æ•£

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ `slot_mapping` æ•°ç»„

**Prefill é˜¶æ®µ**ï¼š`model_runner.py:126-162`

```python
def prepare_prefill(self, seqs: list[Sequence]):
    """å‡†å¤‡ Prefill çš„è¾“å…¥æ•°æ®"""
    slot_mapping = []

    for seq in seqs:
        # è·³è¿‡ç¼“å­˜å‘½ä¸­çš„å—
        for i in range(seq.num_cached_blocks, seq.num_blocks):
            # è®¡ç®—ç‰©ç†å—çš„èµ·å§‹ä½ç½®
            start = seq.block_table[i] * self.block_size

            # è®¡ç®—ç»“æŸä½ç½®
            if i != seq.num_blocks - 1:
                end = start + self.block_size
            else:
                # æœ€åä¸€ä¸ªå—å¯èƒ½æœªæ»¡
                end = start + seq.last_block_num_tokens

            # æ·»åŠ æ‰€æœ‰ token çš„ç‰©ç†ä½ç½®
            slot_mapping.extend(list(range(start, end)))

    return torch.tensor(slot_mapping, dtype=torch.int32)
```

**ç¤ºä¾‹ï¼šslot_mapping çš„æ„å»º**

```python
# åºåˆ—ï¼š1000 tokens
# block_table = [5, 10, 15, 20]

# é€»è¾‘ä½ç½® â†’ ç‰©ç†ä½ç½®
# token 0 â†’ slot 5*256 + 0 = 1280
# token 1 â†’ slot 1281
# ...
# token 255 â†’ slot 1535
# token 256 â†’ slot 10*256 + 0 = 2560
# ...
# token 999 â†’ slot 20*256 + 231 = 5327

slot_mapping = [1280, 1281, ..., 1535, 2560, ..., 5327]
```

**Decode é˜¶æ®µ**ï¼š`model_runner.py:164-180`

```python
def prepare_decode(self, seqs: list[Sequence]):
    """å‡†å¤‡ Decode çš„è¾“å…¥æ•°æ®"""
    slot_mapping = []
    context_lens = []

    for seq in seqs:
        # æ–°ç”Ÿæˆçš„ token çš„ç‰©ç†ä½ç½®
        slot = seq.block_table[-1] * self.block_size + seq.last_block_num_tokens - 1
        slot_mapping.append(slot)

        # åºåˆ—çš„å½“å‰é•¿åº¦ï¼ˆç”¨äº Flash Attentionï¼‰
        context_lens.append(len(seq))

    return torch.tensor(slot_mapping), torch.tensor(context_lens)
```

**ç¤ºä¾‹ï¼šDecode çš„ slot_mapping**

```python
# åºåˆ—å·²ç”Ÿæˆ 1000 tokens
# block_table = [5, 10, 15, 20]
# æ­£åœ¨ç”Ÿæˆç¬¬ 1001 ä¸ª token

# æœ€åä¸€ä¸ªå—æ˜¯ block 20
# å·²ç”¨ä½ç½®ï¼š0-231ï¼ˆ232 ä¸ª tokensï¼‰
# æ–° token çš„ä½ç½®ï¼š20*256 + 232 = 5344

slot_mapping = [5344]
context_lens = [1000]  # Flash Attention éœ€è¦çŸ¥é“åºåˆ—é•¿åº¦
```

### 5.4 å­˜å‚¨åˆ° KV Cache

**æ–‡ä»¶**: `nanovllm/layers/attention.py:10-41`

```python
@triton.jit
def store_kvcache_kernel(
    key_ptr,
    key_stride,
    value_ptr,
    value_stride,
    k_cache_ptr,
    v_cache_ptr,
    slot_mapping_ptr,
    D: tl.constexpr,  # num_heads * head_dim
):
    """å°†æ–°çš„ K å’Œ V å­˜å‚¨åˆ° KV Cache"""
    idx = tl.program_id(0)
    slot = tl.load(slot_mapping_ptr + idx)

    if slot == -1:
        return  # ä¸éœ€è¦å­˜å‚¨ï¼ˆå¦‚ç¼“å­˜å‘½ä¸­çš„å—ï¼‰

    # åŠ è½½æ–°çš„ K å’Œ V
    key_offsets = idx * key_stride + tl.arange(0, D)
    value_offsets = idx * value_stride + tl.arange(0, D)
    key = tl.load(key_ptr + key_offsets)
    value = tl.load(value_ptr + value_offsets)

    # å­˜å‚¨åˆ°å¯¹åº”çš„ç‰©ç†ä½ç½®
    cache_offsets = slot * D + tl.arange(0, D)
    tl.store(k_cache_ptr + cache_offsets, key)
    tl.store(v_cache_ptr + cache_offsets, value)
```

**ä¼˜åŠ¿**ï¼š
- å¹¶è¡Œå†™å…¥ï¼šæ‰€æœ‰ token åŒæ—¶å­˜å‚¨
- éè¿ç»­å­˜å‚¨ï¼šæ”¯æŒåˆ†æ•£çš„ç‰©ç†å—
- é«˜æ•ˆï¼šä½¿ç”¨ Triton GPU kernel

---

## 6. Continuous Batching

### 6.1 Static Batching çš„é—®é¢˜

**ä¼ ç»Ÿæ–¹å¼**ï¼š
```python
batch = [req1, req2, req3, req4]

# å¿…é¡»ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
for step in range(max_steps):
    outputs = model(batch)
    # å³ä½¿ req2 åœ¨ step 10 å°±å®Œæˆäº†ï¼Œ
    # ä¹Ÿè¦ç­‰ req4 åœ¨ step 100 å®Œæˆ
    # æµªè´¹äº†å¤§é‡è®¡ç®—ï¼
```

**å›¾ç¤º**ï¼š
```
æ—¶é—´è½´ â†’

Req1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          (10 steps)
Req2: â–ˆâ–ˆâ–ˆâ–ˆ                (4 steps)  â† æµªè´¹ï¼šç­‰å¾… 6 steps
Req3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              (6 steps)  â† æµªè´¹ï¼šç­‰å¾… 4 steps
Req4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    (14 steps)

æ€»æ—¶é—´ï¼š14 stepsï¼ˆç”±æœ€æ…¢çš„å†³å®šï¼‰
æœ‰æ•ˆè®¡ç®—ï¼š10 + 4 + 6 = 30 steps
æµªè´¹ï¼š14 * 4 - 30 = 26 steps (46%)
```

### 6.2 Continuous Batching çš„ä¼˜åŠ¿

**vLLM æ–¹å¼**ï¼š
```python
while not finished:
    # æ¯æ­¥é‡æ–°è°ƒåº¦
    batch = schedule()

    # æ‰§è¡Œå½“å‰æ‰¹æ¬¡
    outputs = model(batch)

    # ç§»é™¤å®Œæˆçš„è¯·æ±‚
    # æ·»åŠ æ–°çš„è¯·æ±‚
    # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
```

**å›¾ç¤º**ï¼š
```
Step 1: [Req1, Req2, Req3, Req4]
Step 2: [Req1, Req3, Req4]         # Req2 å®Œæˆ
Step 3: [Req1, Req4, Req5]         # Req3 å®Œæˆï¼ŒåŠ å…¥ Req5
Step 4: [Req4, Req5, Req6, Req7]   # Req1 å®Œæˆï¼ŒåŠ å…¥ Req6, Req7
...

æ¯ä¸ªè¯·æ±‚å®Œæˆå°±ç«‹å³ç§»é™¤ï¼Œä¸æµªè´¹è®¡ç®—
```

### 6.3 ä»£ç å®ç°

**æ–‡ä»¶**: `nanovllm/engine/llm_engine.py:48-54`

```python
def step(self):
    """æ‰§è¡Œä¸€ä¸ªæ¨ç†æ­¥éª¤"""
    # 1. è°ƒåº¦ï¼šå†³å®šå“ªäº›è¯·æ±‚æ‰§è¡Œ
    seqs, is_prefill = self.scheduler.schedule()

    # 2. æ‰§è¡Œï¼šPrefill æˆ– Decode
    token_ids = self.model_runner.call("run", seqs, is_prefill)

    # 3. åå¤„ç†ï¼šæ›´æ–°åºåˆ—çŠ¶æ€
    self.scheduler.postprocess(seqs, token_ids)

    # 4. è¿”å›å®Œæˆçš„è¯·æ±‚
    outputs = [(seq.seq_id, seq.completion_token_ids)
               for seq in seqs if seq.is_finished]

    return outputs, num_tokens
```

**ä¸»å¾ªç¯**ï¼š`llm_engine.py:72-89`

```python
while not self.is_finished():
    # æ¯æ­¥éƒ½é‡æ–°è°ƒåº¦
    outputs, num_tokens = self.step()

    # å¤„ç†å®Œæˆçš„è¯·æ±‚
    for seq_id, token_ids in outputs:
        results[seq_id] = token_ids
```

### 6.4 æ€§èƒ½å¯¹æ¯”

**åœºæ™¯**ï¼š256 ä¸ªè¯·æ±‚ï¼Œé•¿åº¦åˆ†å¸ƒ [100, 5000]

| æ–¹å¼ | æ€»æ—¶é—´ | ååé‡ | GPU åˆ©ç”¨ç‡ |
|------|--------|--------|-----------|
| Static Batching | 500s | 2560 tok/s | 40% |
| Continuous Batching | **125s** | **10240 tok/s** | **85%** |

**æå‡**ï¼š4x ååé‡ï¼Œ2x GPU åˆ©ç”¨ç‡

---

## 7. Prefix Caching

### 7.1 åŸç†

**é—®é¢˜**ï¼šå¤šä¸ªè¯·æ±‚æœ‰ç›¸åŒçš„ promptï¼Œé‡å¤è®¡ç®—

**è§£å†³**ï¼šç¼“å­˜å·²è®¡ç®—è¿‡çš„ KV Cache å—

### 7.2 å“ˆå¸Œè®¡ç®—

**æ–‡ä»¶**: `nanovllm/engine/block_manager.py:35-41`

```python
@classmethod
def compute_hash(cls, token_ids: list[int], prefix: int = -1):
    """è®¡ç®—å—çš„å“ˆå¸Œå€¼"""
    h = xxhash.xxh64()  # å¿«é€Ÿå“ˆå¸Œç®—æ³•

    # æ»šåŠ¨å“ˆå¸Œï¼šåŒ…å«å‰ä¸€ä¸ªå—çš„å“ˆå¸Œ
    if prefix != -1:
        h.update(prefix.to_bytes(8, "little"))

    # æ›´æ–°å½“å‰å—çš„ token
    h.update(np.array(token_ids).tobytes())

    return h.intdigest()
```

**ç¤ºä¾‹ï¼šæ»šåŠ¨å“ˆå¸Œ**

```python
# åºåˆ—ï¼š[1, 2, 3, 4, 5, 6, 7, 8]
# å—å¤§å°ï¼š4

# å— 0: [1, 2, 3, 4]
h0 = compute_hash([1, 2, 3, 4], prefix=-1)
# h0 = hash([1, 2, 3, 4])

# å— 1: [5, 6, 7, 8]
h1 = compute_hash([5, 6, 7, 8], prefix=h0)
# h1 = hash(h0 || [5, 6, 7, 8])
#     = hash(hash([1,2,3,4]) || [5,6,7,8])
#     = hash([1,2,3,4,5,6,7,8])
```

**ä¼˜åŠ¿**ï¼šå¯ä»¥å¿«é€Ÿæ£€æµ‹æ•´ä¸ªå‰ç¼€æ˜¯å¦ç›¸åŒ

### 7.3 ç¼“å­˜æŸ¥æ‰¾

**æ–‡ä»¶**: `nanovllm/engine/block_manager.py:59-83`

```python
def allocate(self, seq: Sequence):
    h = -1
    cache_miss = False

    for i in range(seq.num_blocks):
        token_ids = seq.block(i)
        h = self.compute_hash(token_ids, h) if len(token_ids) == self.block_size else -1

        # æŸ¥æ‰¾ç¼“å­˜
        block_id = self.hash_to_block_id.get(h, -1)

        # éªŒè¯ç¼“å­˜ï¼ˆé¿å…å“ˆå¸Œå†²çªï¼‰
        if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
            cache_miss = True

        if cache_miss:
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œåˆ†é…æ–°å—
            block_id = self.free_block_ids[0]
            block = self._allocate_block(block_id)
        else:
            # ç¼“å­˜å‘½ä¸­ï¼
            seq.num_cached_tokens += self.block_size
            block = self.blocks[block_id]
            block.ref_count += 1

        # è®°å½•ç¼“å­˜
        if h != -1:
            block.update(h, token_ids)
            self.hash_to_block_id[h] = block_id

        seq.block_table.append(block_id)
```

**ç¤ºä¾‹ï¼šPrefix Caching**

```python
# è¯·æ±‚ 1
seq1 = Sequence([1, 2, 3, 4, 5, 6, 7, 8])
manager.allocate(seq1)
# å— 0: æ–°åˆ†é… block 0ï¼Œhash = h0
# å— 1: æ–°åˆ†é… block 1ï¼Œhash = h1
# seq1.block_table = [0, 1]

# è¯·æ±‚ 2ï¼ˆå‰åŠéƒ¨åˆ†ç›¸åŒï¼‰
seq2 = Sequence([1, 2, 3, 4, 9, 10, 11, 12])
manager.allocate(seq2)
# å— 0: å‘½ä¸­ç¼“å­˜ï¼å¤ç”¨ block 0
# å— 1: æœªå‘½ä¸­ï¼Œæ–°åˆ†é… block 2
# seq2.block_table = [0, 2]
# seq2.num_cached_tokens = 4 (è·³è¿‡äº† 4 ä¸ª tokens çš„è®¡ç®—)

# èŠ‚çœï¼š50% çš„ Prefill æ—¶é—´
```

### 7.4 æ€§èƒ½æå‡

**åœºæ™¯**ï¼šå¤šç”¨æˆ·ä½¿ç”¨ç›¸åŒçš„ç³»ç»Ÿæç¤ºè¯

```python
system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹..."
user_prompts = ["ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ", ...]

# æ— ç¼“å­˜ï¼š
# Prefill æ—¶é—´ = 1000 * len(system_prompt) ms

# æœ‰ç¼“å­˜ï¼š
# Prefill æ—¶é—´ = 1000 * len(system_prompt) ms (ç¬¬ä¸€ä¸ªè¯·æ±‚)
#             + 100 * len(user_prompt) ms (å…¶ä»–è¯·æ±‚)
# æå‡çº¦ 10x
```

---

## 8. CUDA Graph ä¼˜åŒ–

### 8.1 ä»€ä¹ˆæ˜¯ CUDA Graphï¼Ÿ

**é—®é¢˜**ï¼šæ¯æ¬¡æ¨ç†éƒ½è¦ï¼š
1. CPU å¯åŠ¨ GPU kernels
2. GPU æ‰§è¡Œè®¡ç®—
3. GPU åŒæ­¥

**å¼€é”€**ï¼škernel launch å¯å æ€»æ—¶é—´çš„ 10-20%

**è§£å†³**ï¼šCUDA Graph æ•è·ä¸€æ¬¡è®¡ç®—è¿‡ç¨‹ï¼Œä¹‹ååªéœ€æ›¿æ¢è¾“å…¥æ•°æ®

### 8.2 æ•è·è®¡ç®—å›¾

**æ–‡ä»¶**: `nanovllm/engine/model_runner.py:216-252`

```python
@torch.inference_mode()
def capture_cudagraph(self):
    """æ•è·å¤šç§ batch size çš„ CUDA Graph"""
    max_bs = min(self.config.max_num_seqs, 512)
    max_num_blocks = (config.max_model_len + self.block_size - 1) // self.block_size

    # é¢„åˆ†é…å›ºå®šå†…å­˜
    input_ids = torch.zeros(max_bs, dtype=torch.int64)
    positions = torch.zeros(max_bs, dtype=torch.int64)
    slot_mapping = torch.zeros(max_bs, dtype=torch.int32)
    context_lens = torch.zeros(max_bs, dtype=torch.int32)
    block_tables = torch.zeros(max_bs, max_num_blocks, dtype=torch.int32)
    outputs = torch.zeros(max_bs, hf_config.hidden_size)

    # æ•è·å¤šç§ batch size
    self.graph_bs = [1, 2, 4, 8] + list(range(16, max_bs + 1, 16))
    self.graphs = {}

    for bs in reversed(self.graph_bs):
        graph = torch.cuda.CUDAGraph()

        # è®¾ç½®ä¸Šä¸‹æ–‡
        set_context(
            False,  # Decode æ¨¡å¼
            slot_mapping=slot_mapping[:bs],
            context_lens=context_lens[:bs],
            block_tables=block_tables[:bs]
        )

        # Warmup
        outputs[:bs] = self.model(input_ids[:bs], positions[:bs])

        # æ•è·
        with torch.cuda.graph(graph, pool):
            outputs[:bs] = self.model(input_ids[:bs], positions[:bs])

        # å…±äº«å†…å­˜æ± 
        if self.graph_pool is None:
            self.graph_pool = graph.pool()

        self.graphs[bs] = graph
        torch.cuda.synchronize()
        reset_context()

    # ä¿å­˜å˜é‡å¼•ç”¨
    self.graph_vars = dict(
        input_ids=input_ids,
        positions=positions,
        slot_mapping=slot_mapping,
        context_lens=context_lens,
        block_tables=block_tables,
        outputs=outputs,
    )
```

### 8.3 é‡æ”¾è®¡ç®—å›¾

**æ–‡ä»¶**: `nanovllm/engine/model_runner.py:189-206`

```python
@torch.inference_mode()
def run_model(self, input_ids: torch.Tensor, positions: torch.Tensor, is_prefill: bool):
    if is_prefill or self.enforce_eager or input_ids.size(0) > 512:
        # Prefill æˆ–ç‰¹æ®Šæƒ…å†µï¼šä½¿ç”¨ Eager æ¨¡å¼
        return self.model.compute_logits(self.model(input_ids, positions))
    else:
        # Decodeï¼šä½¿ç”¨ CUDA Graph
        bs = input_ids.size(0)
        context = get_context()

        # é€‰æ‹©æœ€æ¥è¿‘çš„ batch size
        graph = self.graphs[next(x for x in self.graph_bs if x >= bs)]

        # æ›´æ–°è¾“å…¥æ•°æ®ï¼ˆç›´æ¥å†…å­˜æ‹·è´ï¼Œæ—  kernel launchï¼‰
        graph_vars = self.graph_vars
        graph_vars["input_ids"][:bs] = input_ids
        graph_vars["positions"][:bs] = positions
        graph_vars["slot_mapping"].fill_(-1)
        graph_vars["slot_mapping"][:bs] = context.slot_mapping
        graph_vars["context_lens"].zero_()
        graph_vars["context_lens"][:bs] = context.context_lens
        graph_vars["block_tables"][:bs, :context.block_tables.size(1)] = context.block_tables

        # é‡æ”¾å›¾ï¼ˆä¸€æ¬¡ kernel launchï¼‰
        graph.replay()

        return self.model.compute_logits(graph_vars["outputs"][:bs])
```

### 8.4 æ€§èƒ½æå‡

**Decode é˜¶æ®µ**ï¼š
- Eager æ¨¡å¼ï¼š10 msï¼ˆkernel launch: 2 ms + è®¡ç®—: 8 msï¼‰
- CUDA Graphï¼š8.2 msï¼ˆkernel launch: 0.2 ms + è®¡ç®—: 8 msï¼‰
- æå‡ï¼š~18%

**æ³¨æ„**ï¼šåªå¯¹ Decode æœ‰æ•ˆï¼ŒPrefill å› ä¸ºé•¿åº¦ä¸å›ºå®šæ— æ³•ä½¿ç”¨

---

## 9. å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šç†è§£ PagedAttention

**ä»»åŠ¡**ï¼šæ‰‹åŠ¨ç”»å‡ºä»¥ä¸‹åºåˆ—çš„å—åˆ†é…

```python
seq1 = Sequence([1, 2, 3, ..., 1000])  # 1000 tokens
seq2 = Sequence([1, 2, 3, ..., 500, 9999, 9999, ...])  # å‰ 500 ä¸ªç›¸åŒ

manager = BlockManager(num_blocks=100, block_size=256)

manager.allocate(seq1)
manager.allocate(seq2)

# é—®é¢˜ï¼š
# 1. seq1 éœ€è¦å¤šå°‘ä¸ªå—ï¼Ÿ
# 2. seq2 éœ€è¦å¤šå°‘ä¸ªå—ï¼Ÿ
# 3. æœ‰å¤šå°‘ä¸ªå—è¢«å…±äº«ï¼Ÿ
# 4. å¼•ç”¨è®¡æ•°åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿ
```

**ç­”æ¡ˆ**ï¼š
```python
# 1. seq1: (1000 + 256 - 1) // 256 = 4 ä¸ªå—
# 2. seq2: å‡è®¾æ€»é•¿ 1500 tokensï¼Œéœ€è¦ 6 ä¸ªå—
# 3. å‰ 2 ä¸ªå—å…±äº«ï¼ˆ512 tokensï¼‰
# 4. block[0].ref_count = 2, block[1].ref_count = 2
```

### ç»ƒä¹ 2ï¼šç†è§£ Scheduler

**ä»»åŠ¡**ï¼šæ¨¡æ‹Ÿä»¥ä¸‹åœºæ™¯çš„è°ƒåº¦è¿‡ç¨‹

```python
# é…ç½®
max_num_seqs = 4
max_num_batched_tokens = 4096
num_blocks = 100

# è¯·æ±‚é˜Ÿåˆ—
waiting = [
    Sequence([0] * 1000),   # req1: 1000 tokens
    Sequence([0] * 2000),   # req2: 2000 tokens
    Sequence([0] * 500),    # req3: 500 tokens
    Sequence([0] * 3000),   # req4: 3000 tokens
    Sequence([0] * 1000),   # req5: 1000 tokens
]

# é—®é¢˜ï¼š
# 1. ç¬¬1è½®è°ƒåº¦å“ªäº›è¯·æ±‚ï¼Ÿ
# 2. ç¬¬2è½®å‘¢ï¼Ÿ
# 3. å¦‚æœéœ€è¦æŠ¢å ï¼ŒæŠ¢å å“ªä¸ªï¼Ÿ
```

**ç­”æ¡ˆ**ï¼š
```python
# ç¬¬1è½®ï¼šreq1, req2, req3 (1000 + 2000 + 500 = 3500 < 4096)
# ç¬¬2è½®ï¼šreq4 (3000 < 4096, req1, req2, req3 åœ¨ running é˜Ÿåˆ—)
# æŠ¢å ï¼šå¦‚æœèµ„æºä¸è¶³ï¼Œä¼˜å…ˆæŠ¢å æœ€é•¿çš„ï¼ˆreq2 æˆ– req4ï¼‰
```

### ç»ƒä¹ 3ï¼šè®¡ç®— KV Cache å¤§å°

**ä»»åŠ¡**ï¼šè®¡ç®—ä»¥ä¸‹é…ç½®çš„ KV Cache æ˜¾å­˜å ç”¨

```python
num_layers = 32
hidden_size = 4096
num_attention_heads = 32
num_kv_heads = 8
head_dim = 128
block_size = 256
num_blocks = 1000
dtype = torch.float16
```

**ç­”æ¡ˆ**ï¼š
```python
# æ¯ä¸ª KV å¤´çš„ç»´åº¦
head_dim = 128

# æ¯ä¸ªå—çš„å¤§å°ï¼ˆä¸€ä¸ª token çš„ K å’Œ Vï¼‰
bytes_per_token = (
    2 *                     # K å’Œ V
    num_layers *           # 32 å±‚
    num_kv_heads *         # 8 ä¸ª KV å¤´
    head_dim *             # 128 ç»´
    dtype.itemsize         # 2 bytes (float16)
)
# = 2 * 32 * 8 * 128 * 2 = 131,072 bytes

# æ¯ä¸ªå—ï¼ˆ256 ä¸ª tokensï¼‰
bytes_per_block = bytes_per_token * block_size
# = 131,072 * 256 = 33,554,432 bytes â‰ˆ 32 MB

# æ€»æ˜¾å­˜
total_bytes = bytes_per_block * num_blocks
# = 32 MB * 1000 = 32 GB
```

### ç»ƒä¹ 4ï¼šPrefix Caching å‘½ä¸­ç‡

**ä»»åŠ¡**ï¼šè®¡ç®—ä»¥ä¸‹åœºæ™¯çš„ç¼“å­˜å‘½ä¸­ç‡

```python
system_prompt = [1, 2, 3, ..., 500]  # 500 tokens
user_prompts = [
    [100, 101, 102, ..., 200],   # 100 tokens
    [200, 201, 202, ..., 400],   # 200 tokens
    [150, 151, 152, ..., 250],   # 100 tokens
]

# æ‰€æœ‰è¯·æ±‚éƒ½ä½¿ç”¨ç›¸åŒçš„ system_prompt
```

**ç­”æ¡ˆ**ï¼š
```python
# æ€» token æ•°
total_tokens = (500 + 100) + (500 + 200) + (500 + 100) = 1900

# ç¼“å­˜å‘½ä¸­ï¼ˆsystem_prompt çš„ 2 ä¸ªå—ï¼‰
cached_tokens = 500 * 3 = 1500

# å‘½ä¸­ç‡
hit_rate = cached_tokens / (total_tokens + cached_tokens)
        = 1500 / (1900 + 1500)
        = 1500 / 3400
        â‰ˆ 44%

# èŠ‚çœ 44% çš„ Prefill æ—¶é—´
```

### ç»ƒä¹ 5ï¼šContinuous Batching æ€§èƒ½

**ä»»åŠ¡**ï¼šå¯¹æ¯” Static Batching å’Œ Continuous Batching

```python
# è¯·æ±‚ï¼ˆé•¿åº¦ï¼šç”Ÿæˆ token æ•°ï¼‰
requests = [10, 100, 50, 20, 200, 30, 150, 80]

# Static Batchingï¼šæ‰¹æ¬¡å¤§å° 4
# Continuous Batchingï¼šæ¯æ­¥é‡æ–°è°ƒåº¦

# è®¡ç®—ä¸¤ç§æ–¹å¼çš„æ€»æ—¶é—´
```

**ç­”æ¡ˆ**ï¼š
```python
# Static Batching
# Batch 1: [10, 100, 50, 20] â†’ 100 steps
# Batch 2: [200, 30, 150, 80] â†’ 200 steps
# æ€»æ—¶é—´ï¼š300 steps

# Continuous Batchingï¼ˆç®€åŒ–æ¨¡å‹ï¼‰
# Step 1-10:  [10, 100, 50, 20, 200, 30, 150, 80]
# Step 11-20: [100, 50, 200, 30, 150, 80]         # 10 å®Œæˆ
# Step 21-30: [100, 200, 30, 150, 80]            # 20 å®Œæˆ
# Step 31-50: [100, 200, 150, 80]                # 50 å®Œæˆ
# Step 51-80: [100, 200, 150]                    # 30, 80 å®Œæˆ
# Step 81-100: [200, 150]                        # 100 å®Œæˆ
# Step 101-150: [200]                            # 150 å®Œæˆ
# Step 151-200: [200]                            # 200 å®Œæˆ
# æ€»æ—¶é—´ï¼š200 steps

# æå‡ï¼š1.5x
```

---

## 10. æ€»ç»“

### 10.1 æ ¸å¿ƒç»„ä»¶å›é¡¾

| ç»„ä»¶ | è§£å†³çš„é—®é¢˜ | æ ¸å¿ƒæ€æƒ³ | æ€§èƒ½æå‡ |
|------|----------|---------|---------|
| **PagedAttention** | æ˜¾å­˜æµªè´¹ | åˆ†é¡µç®¡ç† | 3x æ˜¾å­˜æ•ˆç‡ |
| **Scheduler** | æ‰¹å¤„ç†æ•ˆç‡ä½ | æŠ¢å å¼è°ƒåº¦ | 5x ååé‡ |
| **Continuous Batching** | è¯·æ±‚ç­‰å¾… | åŠ¨æ€æ‰¹å¤„ç† | 4x ååé‡ |
| **Prefix Caching** | é‡å¤è®¡ç®— | å‰ç¼€å“ˆå¸Œç¼“å­˜ | 10-100x Prefill |
| **CUDA Graph** | Kernel å¼€é”€ | è®¡ç®—å›¾å¤ç”¨ | 1.2x Decode |

### 10.2 é€‚ç”¨åœºæ™¯

**æ¨èä½¿ç”¨ vLLM**ï¼š
- âœ… é«˜å¹¶å‘åœ¨çº¿æœåŠ¡ï¼ˆChatGPTã€å®¢æœæœºå™¨äººï¼‰
- âœ… å¤šç”¨æˆ·å…±äº«ç³»ç»Ÿæç¤ºè¯
- âœ… éœ€è¦é•¿ä¸Šä¸‹æ–‡æ¨ç†
- âœ… å¯¹ååé‡è¦æ±‚é«˜çš„åœºæ™¯

**ä¸æ¨èä½¿ç”¨ vLLM**ï¼š
- âŒ å•æ¬¡ç¦»çº¿æ¨ç†ï¼ˆoverhead å¤ªå¤§ï¼‰
- âŒ æä½å»¶è¿Ÿè¦æ±‚ï¼ˆè°ƒåº¦æœ‰é¢å¤–å¼€é”€ï¼‰
- âŒ è¶…é•¿å•ä¸ªè¯·æ±‚ï¼ˆPrefill æ—¶é—´ä¸»å¯¼ï¼‰

### 10.3 å­¦ä¹ è·¯å¾„

1. **åŸºç¡€**ï¼šç†è§£ KV Cache å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶
2. **æ ¸å¿ƒ**ï¼šæ·±å…¥ PagedAttention å’Œ Scheduler
3. **ä¼˜åŒ–**ï¼šå­¦ä¹  Prefix Caching å’Œ CUDA Graph
4. **å®è·µ**ï¼šè‡ªå·±å®ç°ç®€åŒ–ç‰ˆçš„ vLLM
5. **è¿›é˜¶**ï¼šé˜…è¯»å®Œæ•´ vLLM æºç 

### 10.4 å‚è€ƒèµ„æº

- **nano-vLLM æºç **ï¼šæœ¬é¡¹ç›®çš„æ ¸å¿ƒå®ç°
- **vLLM è®ºæ–‡**ï¼š"Efficient Memory Management for Large Language Model Serving"
- **Flash Attention**ï¼šä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—
- **æ“ä½œç³»ç»Ÿ**ï¼šè™šæ‹Ÿå†…å­˜ã€é¡µè¡¨ã€è°ƒåº¦ç®—æ³•

---

## 11. å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆå—å¤§å°æ˜¯ 256ï¼Ÿ

**A**: å¹³è¡¡è€ƒè™‘ï¼š
- å¤ªå°ï¼ˆå¦‚ 16ï¼‰ï¼šç®¡ç†å¼€é”€å¤§
- å¤ªå¤§ï¼ˆå¦‚ 1024ï¼‰ï¼šç²’åº¦ç²—ï¼Œæµªè´¹å¤š
- 256 æ˜¯ç»éªŒå€¼ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯

### Q2: æŠ¢å ä¼šä¸ä¼šå¯¼è‡´è¯·æ±‚"é¥¥é¥¿"ï¼Ÿ

**A**: ä¼šï¼Œä½†å½±å“æœ‰é™ï¼š
- çŸ­è¯·æ±‚ä¼˜å…ˆå®Œæˆï¼Œé™ä½ P99 å»¶è¿Ÿ
- é•¿è¯·æ±‚è™½ç„¶è¢«æŠ¢å ï¼Œä½†æœ€ç»ˆä¼šå®Œæˆ
- å¯ä»¥é€šè¿‡ä¼˜å…ˆçº§é˜Ÿåˆ—ä¼˜åŒ–

### Q3: Prefix Caching çš„å“ˆå¸Œå†²çªæ€ä¹ˆåŠï¼Ÿ

**A**: ä»£ç ä¸­æœ‰éªŒè¯ï¼š
```python
if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
    cache_miss = True
```
å“ˆå¸Œå†²çªæ—¶ä¼šé‡æ–°è®¡ç®—ï¼Œä¿è¯æ­£ç¡®æ€§

### Q4: CUDA Graph ä¸ºä»€ä¹ˆä¸èƒ½ç”¨äº Prefillï¼Ÿ

**A**: Prefill çš„è¾“å…¥é•¿åº¦ä¸å›ºå®šï¼š
- æ¯ä¸ª prompt é•¿åº¦ä¸åŒ
- CUDA Graph éœ€è¦å›ºå®šçš„å¼ é‡å½¢çŠ¶
- åªèƒ½ç”¨äº Decodeï¼ˆæ¯æ¬¡éƒ½æ˜¯ 1 ä¸ª tokenï¼‰

### Q5: Tensor Parallelism æ€ä¹ˆå·¥ä½œï¼Ÿ

**A**: æ¨¡å‹å¹¶è¡Œï¼š
- å°†æ¨¡å‹åˆ‡åˆ†åˆ°å¤šä¸ª GPU
- æ¯ä¸ª GPU è®¡ç®—ä¸€éƒ¨åˆ†å¤´
- é€šè¿‡ all-reduce é€šä¿¡èšåˆç»“æœ
- çº¿æ€§æ‰©å±•æ€§èƒ½

---

**æ­å–œä½ å®Œæˆäº† vLLM æ·±åº¦å­¦ä¹ ï¼**

æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿ç»§ç»­äº¤æµ ğŸ‰
